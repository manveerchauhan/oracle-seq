# Modified Statistical Analysis of Rarefaction Curves with Marginal Returns Approach
# Script to fit mathematical models to rarefaction curves and analyze marginal returns
# for determining practical saturation points

library(tidyverse)
library(broom)
library(patchwork)
library(gridExtra)
library(minpack.lm)  # for nonlinear least squares fitting
library(viridis)     # for color palettes
library(kableExtra)  # for nice tables
library(ggrepel)     # for better text positioning
library(ggsignif)    # for significance brackets
library(cowplot)     # for get_legend function
library(tidytext)    # for reorder_within function
library(parallel)    # For parallel bootstrap
library(ggdist)      # For uncertainty visualization

# Set working directory to the pipeline location
setwd("/data/gpfs/projects/punim2251/Aim1_LongBench/longbench-analysis-pipeline")

theme_set(theme_minimal())

# Load data generated by generate_rarefaction_curve.R
cat("=== LOADING RAREFACTION DATA FOR STATISTICAL ANALYSIS ===\n")

# Check if data files exist
stats_dir <- "statistical_analysis"
data_file <- file.path(stats_dir, "rarefaction_data_for_stats.rds")
metadata_file <- file.path(stats_dir, "curve_metadata.rds")

if (!file.exists(data_file)) {
  stop("Data file not found. Please run generate_rarefaction_curve.R first.")
}

# Load data
rarefaction_data <- readRDS(data_file)
curve_metadata <- readRDS(metadata_file)

cat("Loaded data with", nrow(rarefaction_data), "data points and", 
    length(unique(rarefaction_data$curve_id)), "curves\n")

# Debug: Check for all expected curves
expected_curves <- c(
  "ONT_Bulk_Genes Discovery", "ONT_Bulk_Isoforms Discovery",
  "ONT_SC_Genes Discovery", "ONT_SC_Isoforms Discovery", 
  "ONT_SN_Genes Discovery", "ONT_SN_Isoforms Discovery",
  "PacBio_Bulk_Genes Discovery", "PacBio_Bulk_Isoforms Discovery",
  "PacBio_SC_Genes Discovery", "PacBio_SC_Isoforms Discovery",
  "PacBio_SN_Genes Discovery", "PacBio_SN_Isoforms Discovery"
)

actual_curves <- unique(rarefaction_data$curve_id)
missing_curves <- setdiff(expected_curves, actual_curves)
unexpected_curves <- setdiff(actual_curves, expected_curves)

cat("Expected curves:", length(expected_curves), "\n")
cat("Actual curves found:", length(actual_curves), "\n")
if(length(missing_curves) > 0) {
  cat("MISSING curves:\n")
  cat(paste("-", missing_curves, collapse = "\n"), "\n")
}
if(length(unexpected_curves) > 0) {
  cat("UNEXPECTED curves (check naming):\n") 
  cat(paste("-", unexpected_curves, collapse = "\n"), "\n")
}

cat("All actual curve IDs:\n")
cat(paste("-", sort(actual_curves), collapse = "\n"), "\n")

# =============================================================================
# CONFIGURATION: Plot-Specific Color Schemes
# =============================================================================

# Plot-specific color scheme configuration
PLOT_COLOR_SCHEMES <- list(
  conversion_plot = "PRIMARY_OVERLAP",     # Red + Blue for overlap detection
  individual_fits = "STANDARD",           # Current colors
  extended_curves = "STANDARD", 
  marginal_returns = "STANDARD",
  threshold_plots = "STANDARD",
  bootstrap_plots = "STANDARD"
)

# Color scheme definitions
get_color_map <- function(plot_type = "default") {
  scheme <- PLOT_COLOR_SCHEMES[[plot_type]] %||% "STANDARD"
  
  switch(scheme,
    "STANDARD" = c("PacBio" = "#DA1884", "ONT" = "#003F5C"),           # Current colors
    "PRIMARY_OVERLAP" = c("PacBio" = "#E31A1C", "ONT" = "#1F78B4")     # Red + Blue for overlap detection
  )
}

# Default color mapping for technologies and sample types (for backward compatibility)
color_map <- get_color_map("default")

# Line type mapping for sample types
linetype_map <- c(
  "Bulk" = "solid",
  "SC" = "dashed", 
  "SN" = "dotted"
)

# Point shape mapping for sample types
shape_map <- c(
  "SC" = 16,    # solid circle
  "SN" = 17,    # solid triangle
  "Bulk" = 15   # solid square
)

# Define marginal return thresholds
MARGINAL_THRESHOLDS <- c(10, 1, 0.1)

# Set bootstrap parameters
N_BOOTSTRAP <- 1000  # Number of bootstrap iterations
CONFIDENCE_LEVEL <- 0.95  # For confidence intervals
set.seed(42)  # For reproducibility

# Safe parameter getter with multiple name options (global scope)
safe_param_get <- function(params, param_names, default = NA) {
  for (name in param_names) {
    if (!is.null(names(params)) && name %in% names(params)) {
      return(as.numeric(params[[name]]))
    }
  }
  # Try positional if all names fail
  if (length(params) >= 1) {
    return(as.numeric(params[[1]]))
  }
  return(default)
}

# Debug: Check data structure
cat("Debugging data structure:\n")
cat("Curve IDs found:", length(unique(rarefaction_data$curve_id)), "curves\n")

## Create Total Reads to Median Reads Per Cell Conversion Function -------
cat("\n=== CREATING READS PER CELL CONVERSION FUNCTION ===\n")

# Analyze the relationship between total reads and median reads per cell
create_reads_per_cell_converter <- function(rarefaction_data) {
  # Filter to only data points with median_reads_per_cell values (SC/SN data)
  cell_data <- rarefaction_data %>%
    filter(!is.na(median_reads_per_cell), median_reads_per_cell > 0) %>%
    select(total_reads_unified, median_reads_per_cell, curve_id)
  
  cat("Found", nrow(cell_data), "data points with reads per cell information\n")
  
  if (nrow(cell_data) < 5) {
    cat("WARNING: Too few data points for reliable conversion. Using fallback.\n")
    # Return a simple fallback function
    return(function(total_reads) total_reads / 1000)  # Rough estimate
  }
  
  # Debug: show the relationship
  cat("Total reads range:", scales::comma(range(cell_data$total_reads_unified)), "\n")
  cat("Median reads per cell range:", range(cell_data$median_reads_per_cell), "\n")
  
  # Fit linear model: median_reads_per_cell ~ total_reads_unified
  conversion_model <- lm(median_reads_per_cell ~ total_reads_unified, data = cell_data)
  
  # Check model quality
  r_squared <- summary(conversion_model)$r.squared
  cat("Linear model R-squared:", round(r_squared, 3), "\n")
  
  # Extract coefficients
  intercept <- coef(conversion_model)[1]
  slope <- coef(conversion_model)[2]
  
  cat("Model: median_reads_per_cell =", round(intercept, 3), "+", round(slope, 6), "* total_reads\n")
  
  # Create and return the conversion function
  conversion_function <- function(total_reads) {
    estimated_reads_per_cell <- intercept + slope * total_reads
    # Ensure positive values
    pmax(estimated_reads_per_cell, 0)
  }
  
  # Store model info as attributes
  attr(conversion_function, "model") <- conversion_model
  attr(conversion_function, "r_squared") <- r_squared
  attr(conversion_function, "intercept") <- intercept
  attr(conversion_function, "slope") <- slope
  
  return(conversion_function)
}

# Create the conversion function
reads_per_cell_converter <- create_reads_per_cell_converter(rarefaction_data)

## Create Conversion Model Visualization -------
cat("\n=== CREATING READS CONVERSION MODEL PLOT ===\n")

create_conversion_model_plot <- function(rarefaction_data, reads_per_cell_converter) {
  # Extract the data used for conversion model
  conversion_data <- rarefaction_data %>%
    filter(!is.na(median_reads_per_cell), median_reads_per_cell > 0) %>%
    mutate(
      technology = str_extract(curve_id, "^[^_]+"),
      sample_type = str_extract(curve_id, "(?<=_)[^_]+(?=_)"),
      total_reads_millions = total_reads_unified / 1e6
    ) %>%
    select(curve_id, technology, sample_type, total_reads_unified, total_reads_millions, median_reads_per_cell)
  
  # Get model statistics
  model_r_squared <- attr(reads_per_cell_converter, "r_squared")
  model_intercept <- attr(reads_per_cell_converter, "intercept")
  model_slope <- attr(reads_per_cell_converter, "slope")
  
  # Create fitted line data
  x_range <- range(conversion_data$total_reads_unified)
  x_seq <- seq(x_range[1], x_range[2], length.out = 100)
  fitted_line <- data.frame(
    total_reads_unified = x_seq,
    total_reads_millions = x_seq / 1e6,
    fitted_reads_per_cell = reads_per_cell_converter(x_seq)
  )
  
  # Format equation text
  equation_text <- paste0(
    "y = ", round(model_intercept, 3), 
    " + ", format(model_slope, scientific = TRUE, digits = 3), 
    " × x"
  )
  
  # Create quality assessment
  quality_text <- case_when(
    model_r_squared >= 0.9 ~ "Excellent",
    model_r_squared >= 0.8 ~ "Good", 
    model_r_squared >= 0.7 ~ "Fair",
    TRUE ~ "Poor"
  )
  
  # Get plot-specific color mapping
  plot_colors <- get_color_map("STANDARD")
  
  # Calculate n per technology (corrected for pseudoreplication)
  tech_summary <- conversion_data %>%
    group_by(technology) %>%
    summarise(n_points = n() / 2, .groups = 'drop')  # Divide by 2 for pseudoreplication
  
  # Create the plot
  p <- ggplot() +
    # Add fitted line
    geom_line(data = fitted_line, 
              aes(x = total_reads_millions, y = fitted_reads_per_cell),
              color = "black", linewidth = 1.2, alpha = 0.8) +
    # Add data points colored by technology
    geom_point(data = conversion_data,
               aes(x = total_reads_millions, y = median_reads_per_cell, 
                   color = technology, shape = sample_type),
               size = 3, alpha = 0.6) +
    # Facet by technology for clear separation
    facet_wrap(~technology, scales = "free") +
    # Color and shape mappings
    scale_color_manual(values = plot_colors, name = "Technology") +
    scale_shape_manual(values = c("SC" = 16, "SN" = 17), 
                      name = "Sample Type",
                      labels = c("SC" = "Single-Cell", "SN" = "Single-Nucleus")) +
    # Add technology-specific model statistics annotation
    geom_label(data = tech_summary,
               aes(x = Inf, y = Inf, label = paste0(
                 equation_text, "\n",
                 "R² = ", round(model_r_squared, 3), "\n",
                 "n = ", n_points, " rarefaction data points"
               )),
               hjust = 1, vjust = 1, size = 4, 
               fill = "white", alpha = 0.8, label.padding = unit(0.5, "lines")) +
    # Axis formatting
    scale_x_continuous(
      name = "Total Sequencing Depth (Million Reads)",
      labels = scales::comma,
      breaks = scales::pretty_breaks(n = 6)
    ) +
    scale_y_continuous(
      name = "Measured Median Reads per Cell", 
      labels = scales::comma,
      breaks = scales::pretty_breaks(n = 6)
    ) +
    # Labels and theme
    labs(
      title = "Total Reads to Reads per Cell Conversion Model",
      subtitle = "Linear regression fitted to single-cell and single-nucleus data with measured reads per cell",
      caption = "Used for converting total sequencing depth to estimated per-cell depth across all protocols"
    ) +
    theme_minimal() +
    theme(
      text = element_text(size = 11),
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 10, color = "gray50"),
      plot.caption = element_text(size = 9, color = "gray60", hjust = 0),
      legend.position = "bottom",
      legend.box = "horizontal",
      legend.title = element_text(size = 10, face = "bold"),
      legend.text = element_text(size = 9),
      panel.grid.minor = element_blank()
    ) +
    guides(
      color = guide_legend(title = "Technology", override.aes = list(size = 4)),
      shape = guide_legend(title = "Sample Type", override.aes = list(size = 4))
    )
  
  return(p)
}

# Create and save the conversion model plot
conversion_plot <- create_conversion_model_plot(rarefaction_data, reads_per_cell_converter)

conversion_plot_pdf <- file.path(plots_dir, "reads_conversion_model.pdf")
ggsave(conversion_plot_pdf, plot = conversion_plot, width = 10, height = 8, device = "pdf")

cat("Reads conversion model plot saved to:", conversion_plot_pdf, "\n")
cat("Model equation: median_reads_per_cell =", round(conversion_intercept, 3), "+", 
    format(conversion_slope, scientific = TRUE, digits = 3), "* total_reads\n")
cat("Model R-squared:", round(conversion_r_squared, 3), "\n")
cat("Data points used:", cell_data_count, "\n")

## Model Fitting Functions -------

# Helper function to generate multiple starting value sets
generate_starting_values <- function(x, y, model_type) {
  max_y <- max(y, na.rm = TRUE)
  min_y <- min(y, na.rm = TRUE)
  range_y <- max_y - min_y
  max_x <- max(x, na.rm = TRUE)
  min_x <- min(x, na.rm = TRUE)
  median_x <- median(x, na.rm = TRUE)
  
  if (model_type == "michaelis_menten") {
    # y = (a * x) / (b + x)
    half_max_x <- x[which.min(abs(y - max_y/2))]
    list(
      conservative = list(a = max_y * 0.8, b = median_x),
      optimistic = list(a = max_y * 1.2, b = max_x * 0.3),
      data_driven = list(a = quantile(y, 0.9), b = half_max_x)
    )
  } else if (model_type == "asymptotic_exp") {
    # y = a * (1 - exp(-b * x))
    initial_slope <- (y[2] - y[1]) / (x[2] - x[1])
    b_start <- abs(initial_slope / max_y)
    list(
      conservative = list(a = max_y * 0.8, b = b_start * 0.5),
      optimistic = list(a = max_y * 1.2, b = b_start * 2),
      data_driven = list(a = quantile(y, 0.9), b = b_start)
    )
  } else if (model_type == "power_law") {
    # y = a * x^b
    list(
      conservative = list(a = max_y / (max_x^0.5), b = 0.3),
      optimistic = list(a = max_y / (max_x^0.7), b = 0.7),
      data_driven = list(a = y[1] / x[1], b = 0.5)
    )
  } else if (model_type == "logarithmic") {
    # y = a * log(x) + b
    list(
      conservative = list(a = range_y / log(max_x/min_x) * 0.8, b = min_y),
      optimistic = list(a = range_y / log(max_x/min_x) * 1.2, b = min_y),
      data_driven = list(a = (max_y - min_y) / (log(max_x) - log(min_x)), b = min_y)
    )
  } else if (model_type == "shifted_logarithmic") {
    # y = a * log(x + c) + b
    list(
      conservative = list(a = range_y / log(max_x), b = min_y, c = 1),
      optimistic = list(a = range_y / log(max_x + min_x), b = min_y, c = min_x),
      data_driven = list(a = range_y / log(max_x), b = y[1], c = 0.1)
    )
  } else if (model_type == "hill") {
    # y = (a * x^n) / (b^n + x^n)
    half_max_x <- x[which.min(abs(y - max_y/2))]
    list(
      conservative = list(a = max_y * 0.9, b = half_max_x, n = 1),
      optimistic = list(a = max_y * 1.1, b = half_max_x * 0.8, n = 2),
      data_driven = list(a = max_y, b = median_x, n = 1.5)
    )
  }
}

# Generic robust fitting function
fit_model_robust <- function(x, y, model_type, curve_name = "unknown") {
  starting_sets <- generate_starting_values(x, y, model_type)
  best_fit <- NULL
  best_aic <- Inf
  
  # Define model formulas
  formulas <- list(
    michaelis_menten = y ~ (a * x) / (b + x),
    asymptotic_exp = y ~ a * (1 - exp(-b * x)),
    power_law = y ~ a * x^b,
    logarithmic = y ~ a * log(x) + b,
    shifted_logarithmic = y ~ a * log(x + c) + b,
    hill = y ~ (a * x^n) / (b^n + x^n)
  )
  
  # Special handling for models requiring positive x values
  if (model_type %in% c("power_law", "logarithmic")) {
    valid_indices <- x > 0 & y > 0
    if (sum(valid_indices) < length(x) * 0.8) {
      return(list(
        model = model_type,
        curve_name = curve_name,
        convergence = FALSE,
        aic = NA,
        error = "Too many zero/negative values"
      ))
    }
    x_fit <- x[valid_indices]
    y_fit <- y[valid_indices]
  } else {
    x_fit <- x
    y_fit <- y
  }
  
  # Try different starting values
  for (start_name in names(starting_sets)) {
    start_params <- starting_sets[[start_name]]
    
    tryCatch({
      fit <- nlsLM(formulas[[model_type]], 
                   start = start_params,
                   data = data.frame(x = x_fit, y = y_fit),
                   control = nls.lm.control(maxiter = 1000))
      
      # Calculate AIC
      aic_value <- AIC(fit)
      
      if (aic_value < best_aic) {
        best_aic <- aic_value
        best_fit <- fit
      }
      
    }, error = function(e) {
      # Continue to next starting value set
    })
  }
  
  if (is.null(best_fit)) {
    return(list(
      model = model_type,
      curve_name = curve_name,
      convergence = FALSE,
      aic = NA,
      error = "All starting value sets failed"
    ))
  }
  
  # Extract parameters and statistics
  params <- tidy(best_fit)
  fit_stats <- glance(best_fit)
  
  # Calculate predictions and residuals
  if (model_type %in% c("power_law", "logarithmic")) {
    pred_y_fit <- predict(best_fit)
    pred_y <- rep(NA, length(x))
    pred_y[valid_indices] <- pred_y_fit
    residuals <- y - pred_y
  } else {
    pred_y <- predict(best_fit, newdata = data.frame(x = x))
    residuals <- y - pred_y
  }
  
  r_squared <- 1 - sum(residuals^2, na.rm = TRUE) / sum((y - mean(y))^2)
  
  # Extract key parameters based on model type
  coef_values <- coef(best_fit)
  result <- list(
    model = model_type,
    curve_name = curve_name,
    fit_object = best_fit,
    parameters = params,
    fit_stats = fit_stats,
    r_squared = r_squared,
    aic = best_aic,
    convergence = TRUE,
    predictions = pred_y,
    residuals = residuals,
    x_values = x,
    y_values = y
  )
  
  # Add model-specific parameters for later use - robust extraction
  # Helper function for robust coefficient extraction
  safe_coef_extract <- function(coef_values, param_name, position) {
    # Try named extraction first
    if (!is.null(names(coef_values)) && param_name %in% names(coef_values)) {
      return(as.numeric(coef_values[param_name]))
    }
    # Fall back to positional extraction
    if (length(coef_values) >= position) {
      return(as.numeric(coef_values[position]))
    }
    # Last resort: return NA
    return(NA)
  }
  
  if (model_type == "michaelis_menten") {
    result$a <- safe_coef_extract(coef_values, "a", 1)
    result$b <- safe_coef_extract(coef_values, "b", 2)
  } else if (model_type == "asymptotic_exp") {
    result$a <- safe_coef_extract(coef_values, "a", 1)
    result$b <- safe_coef_extract(coef_values, "b", 2)
  } else if (model_type == "hill") {
    result$a <- safe_coef_extract(coef_values, "a", 1)
    result$b <- safe_coef_extract(coef_values, "b", 2)
    result$n <- safe_coef_extract(coef_values, "n", 3)
  } else if (model_type == "power_law") {
    result$a <- safe_coef_extract(coef_values, "a", 1)
    result$b <- safe_coef_extract(coef_values, "b", 2)
  } else if (model_type == "logarithmic") {
    result$a <- safe_coef_extract(coef_values, "a", 1)
    result$b <- safe_coef_extract(coef_values, "b", 2)
  } else if (model_type == "shifted_logarithmic") {
    result$a <- safe_coef_extract(coef_values, "a", 1)
    result$b <- safe_coef_extract(coef_values, "b", 2)
    result$c <- safe_coef_extract(coef_values, "c", 3)
  }
  
  return(result)
}

# Function to fit all models and select best based on AIC
fit_best_model_aic <- function(data_subset) {
  curve_name <- unique(data_subset$curve_id)[1]
  x <- data_subset$total_reads_unified
  y <- data_subset$featureNum
  
  # Debug: Check for empty curve_name
  if (is.na(curve_name) || curve_name == "" || length(curve_name) == 0) {
    cat("Warning: Empty curve_name detected. Data subset:\n")
    print(data_subset)
    curve_name <- "unknown_curve"
  }
  
  # Improved debug output with explicit flushing
  cat("Fitting models for: '", curve_name, "'\n", sep = "")
  flush.console()  # Force immediate output
  
  # Debug: Check data quality
  if (length(x) == 0 || length(y) == 0) {
    cat("Error: No data points for curve:", curve_name, "\n")
    return(list(
      model = "no_data",
      curve_name = curve_name,
      convergence = FALSE,
      aic = NA,
      error = "No data points"
    ))
  }
  
  cat("  Data points:", length(x), "| X range:", min(x, na.rm = TRUE), "-", max(x, na.rm = TRUE), 
      "| Y range:", min(y, na.rm = TRUE), "-", max(y, na.rm = TRUE), "\n")
  flush.console()
  
  # Define models to fit
  models_to_fit <- c("michaelis_menten", "asymptotic_exp", "power_law", 
                     "logarithmic", "shifted_logarithmic", "hill")
  
  # Fit all models
  all_fits <- lapply(models_to_fit, function(model_type) {
    fit_model_robust(x, y, model_type, curve_name)
  })
  names(all_fits) <- models_to_fit
  
  # Collect converged models
  converged_models <- all_fits[sapply(all_fits, function(m) m$convergence)]
  
  # Select best model based on AIC
  if (length(converged_models) == 0) {
    # Fallback to linear model
    lm_fit <- lm(y ~ x)
    best_fit <- list(
      model = "linear_fallback",
      curve_name = curve_name,
      fit_object = lm_fit,
      r_squared = summary(lm_fit)$r.squared,
      aic = AIC(lm_fit),
      convergence = TRUE,
      predictions = predict(lm_fit),
      residuals = residuals(lm_fit),
      x_values = x,
      y_values = y
    )
  } else {
    # Find model with lowest AIC
    aic_values <- sapply(converged_models, function(m) m$aic)
    best_model_name <- names(which.min(aic_values))
    best_fit <- converged_models[[best_model_name]]
    
    # Report AIC differences
    min_aic <- min(aic_values)
    aic_diffs <- aic_values - min_aic
    
    # Models with delta AIC < 2 are considered competitive
    competitive_models <- names(aic_diffs[aic_diffs < 2])
    best_fit$model_uncertainty <- ifelse(length(competitive_models) > 1, 
                                         "Multiple competitive models", 
                                         "Clear best model")
    best_fit$aic_differences <- aic_diffs
  }
  
  # Store all model results for comparison
  best_fit$all_models <- all_fits
  
  return(best_fit)
}

## Marginal Returns Functions (Simplified & Robust) -------

# Simple, robust function to calculate marginal returns without complex derivatives
calculate_marginal_returns_simple <- function(model_result, x_eval = NULL) {
  if (is.null(x_eval)) {
    x_range <- range(model_result$x_values)
    x_eval <- 10^seq(log10(x_range[1]), log10(x_range[2] * 100), length.out = 1000)
  }
  
  # Use simple finite differences for derivatives - much more robust
  tryCatch({
    # Get model predictions
    y_pred <- predict(model_result$fit_object, newdata = data.frame(x = x_eval))
    
    # Remove any problematic points
    valid_idx <- is.finite(x_eval) & is.finite(y_pred) & x_eval > 0 & y_pred > 0
    x_clean <- x_eval[valid_idx]
    y_clean <- y_pred[valid_idx]
    
    if (length(x_clean) < 10) {
      stop("Insufficient valid points")
    }
    
    # Simple finite differences for derivative
    dx <- diff(x_clean)
    dy <- diff(y_clean)
    dy_dx <- dy / dx
    
    # Convert to features per million reads
    marginal_returns <- dy_dx * 1e6
    
    # Use midpoints for x values
    x_mid <- x_clean[-length(x_clean)] + dx/2
    
    # Ensure all values are positive and finite
    valid_mr <- is.finite(marginal_returns) & marginal_returns > 0
    
    result_df <- data.frame(
      x = x_mid[valid_mr],
      marginal_returns = marginal_returns[valid_mr]
    )
    
    # Sort and ensure no gaps
    result_df <- result_df[order(result_df$x), ]
    
    return(result_df)
    
  }, error = function(e) {
    # Robust fallback - create reasonable declining marginal returns
    x_scaled <- (x_eval - min(x_eval)) / (max(x_eval) - min(x_eval))
    
    # Start high and decline based on the curve characteristics
    max_y <- max(model_result$y_values, na.rm = TRUE)
    min_x <- min(model_result$x_values, na.rm = TRUE)
    initial_rate <- max_y / min_x * 1e6  # Convert to per million
    
    # Create plausible declining marginal returns
    fallback_mr <- pmax(initial_rate * (1 - x_scaled)^0.6, 0.01)
    
    return(data.frame(
      x = x_eval,
      marginal_returns = fallback_mr
    ))
  })
}

# CORRECT mathematical approach: solve f'(x) = threshold using actual fitted functions
find_threshold_depths <- function(model_result, thresholds = MARGINAL_THRESHOLDS) {
  
  threshold_depths <- data.frame()
  model_type <- model_result$model
  
  # Get model parameters (extracted during fitting)
  params <- coef(model_result$fit_object)
  
  cat("  Model:", model_type, "| Parameters:", paste(names(params), "=", round(params, 4), collapse = ", "), "\n")
  
  # DEBUG: Track successful calculations
  successful_calculations <- c()
  failed_calculations <- c()
  
  for (thresh in thresholds) {
    depth <- NA
    features <- NA
    calculation_successful <- FALSE
    failure_reason <- "Unknown"
    
    # Solve f'(x) = threshold analytically for each model type
    tryCatch({
      
      if (model_type == "michaelis_menten") {
        # f(x) = (a * x) / (b + x)
        # f'(x) = (a * b) / (b + x)^2
        # Solve: (a * b) / (b + x)^2 = thresh / 1e6
        a <- params[["a"]] %||% params[[1]]
        b <- params[["b"]] %||% params[[2]]
        
        # Rearrange: (b + x)^2 = (a * b * 1e6) / thresh
        # x = sqrt((a * b * 1e6) / thresh) - b
        depth <- sqrt((a * b * 1e6) / thresh) - b
        
      } else if (model_type == "asymptotic_exp") {
        # f(x) = a * (1 - exp(-b * x))
        # f'(x) = a * b * exp(-b * x)
        # Solve: a * b * exp(-b * x) = thresh / 1e6
        # Handle multiple possible parameter names and missing parameters
        a <- safe_param_get(params, c("a", "a.90%"), params[[1]])
        b <- safe_param_get(params, c("b"), ifelse(length(params) >= 2, params[[2]], 0))
        
        # Special case: if b is very small or zero (essentially flat curve), use conservative estimates
        if (is.na(b) || abs(b) < 1e-10) {
          x_max <- max(model_result$x_values)
          depth <- x_max * ifelse(thresh == 10, 2, ifelse(thresh == 1, 5, 15))
        } else {
          # Rearrange: exp(-b * x) = (thresh / 1e6) / (a * b)
          # x = -ln((thresh / 1e6) / (a * b)) / b
          depth <- -log((thresh / 1e6) / (a * b)) / b
        }
        
      } else if (model_type == "power_law") {
        # f(x) = a * x^b
        # f'(x) = a * b * x^(b-1)
        # Solve: a * b * x^(b-1) = thresh / 1e6
        a <- params[["a"]] %||% params[[1]]
        b <- params[["b"]] %||% params[[2]]
        
        # x^(b-1) = (thresh / 1e6) / (a * b)
        # x = ((thresh / 1e6) / (a * b))^(1/(b-1))
        if (b != 1) {
          depth <- ((thresh / 1e6) / (a * b))^(1/(b-1))
        }
        
      } else if (model_type == "logarithmic") {
        # f(x) = a * log(x) + b
        # f'(x) = a / x
        # Solve: a / x = thresh / 1e6
        a <- params[["a"]] %||% params[[1]]
        
        # x = a * 1e6 / thresh
        depth <- a * 1e6 / thresh
        
      } else if (model_type == "shifted_logarithmic") {
        # f(x) = a * log(x + c) + b
        # f'(x) = a / (x + c)
        # Solve: a / (x + c) = thresh / 1e6
        a <- params[["a"]] %||% params[[1]]
        c <- params[["c"]] %||% params[[3]]
        
        # x + c = a * 1e6 / thresh
        # x = a * 1e6 / thresh - c
        depth <- a * 1e6 / thresh - c
        
      } else if (model_type == "hill") {
        # f(x) = (a * x^n) / (b^n + x^n)
        # f'(x) = (a * n * b^n * x^(n-1)) / (b^n + x^n)^2
        # Solve: (a * n * b^n * x^(n-1)) / (b^n + x^n)^2 = thresh / 1e6
        a <- params[["a"]] %||% params[[1]]
        b <- params[["b"]] %||% params[[2]]
        n <- params[["n"]] %||% params[[3]]
        
        # This is a complex equation to solve analytically, but we can use numerical methods
        # Define the equation to solve: f'(x) - threshold/1e6 = 0
        hill_derivative_eq <- function(x) {
          if (x <= 0) return(Inf)  # Avoid negative x values
          derivative <- (a * n * b^n * x^(n-1)) / (b^n + x^n)^2
          return(derivative - thresh / 1e6)
        }
        
        # Use numerical root finding to solve f'(x) = threshold/1e6
        # Start search from current max depth and extend outward
        x_start <- max(model_result$x_values)
        x_upper <- x_start * 100  # Search up to 100x current depth
        
        # Try to find root using uniroot
        tryCatch({
          # Check if the function changes sign over the interval
          f_start <- hill_derivative_eq(x_start)
          f_upper <- hill_derivative_eq(x_upper)
          
          if (f_start * f_upper < 0) {
            # Root exists in the interval
            root_result <- uniroot(hill_derivative_eq, c(x_start, x_upper))
            depth <- root_result$root
          } else {
            # Try different bounds - Hill curves often need higher upper bounds
            x_upper <- x_start * 1000
            f_upper <- hill_derivative_eq(x_upper)
            
            if (f_start * f_upper < 0) {
              root_result <- uniroot(hill_derivative_eq, c(x_start, x_upper))
              depth <- root_result$root
            } else {
              # If still no sign change, the threshold may never be reached
              depth <- NA
            }
          }
        }, error = function(e) {
          # If numerical method fails, fall back to NA
          depth <- NA
        })
        
      } else {
        # Fallback: use numerical search for other model types
        depth <- NA
      }
      
      # Validate depth is positive and reasonable
      if (!is.na(depth) && depth > 0 && is.finite(depth)) {
        # Calculate features at this depth using the fitted function
        features <- predict(model_result$fit_object, newdata = data.frame(x = depth))
        features <- as.numeric(features)
        
        # Additional validation
        if (is.finite(features) && features > 0) {
          threshold_depths <- rbind(threshold_depths, data.frame(
            threshold = thresh,
            depth = depth,
            features = features,
            fold_increase_from_current = depth / max(model_result$x_values)
          ))
          
          calculation_successful <- TRUE
          successful_calculations <- c(successful_calculations, paste0(thresh, "f/M"))
          cat("    ✓ ", thresh, "f/M: depth =", round(depth), "reads, features =", round(features), "\n")
        } else {
          failure_reason <- paste("invalid features: features =", features)
          failed_calculations <- c(failed_calculations, paste0(thresh, "f/M: ", failure_reason))
          cat("    ✗ ", thresh, "f/M: mathematical solution failed (invalid features: ", features, ")\n")
        }
      } else {
        failure_reason <- paste("invalid depth: depth =", depth)
        failed_calculations <- c(failed_calculations, paste0(thresh, "f/M: ", failure_reason))
        cat("    ✗ ", thresh, "f/M: mathematical solution failed (invalid depth: ", depth, ")\n")
      }
      
    }, error = function(e) {
      failure_reason <- paste("error in calculation:", e$message)
      failed_calculations <- c(failed_calculations, paste0(thresh, "f/M: ", failure_reason))
      cat("    ✗ Failed to solve", thresh, "f/M threshold:", e$message, "\n")
    })
  }
  
  # DEBUG: Summary of calculations
  cat("  CALCULATION SUMMARY:\n")
  if (length(successful_calculations) > 0) {
    cat("    ✓ Successful:", paste(successful_calculations, collapse = ", "), "\n")
  }
  if (length(failed_calculations) > 0) {
    cat("    ✗ Failed:", paste(failed_calculations, collapse = "; "), "\n")
  }
  
  # FALLBACK: Ensure all thresholds have entries (for protocols that can't reach low thresholds mathematically)
  missing_thresholds <- setdiff(thresholds, threshold_depths$threshold)
  
  if (length(missing_thresholds) > 0) {
    cat("  Adding fallback estimates for missing thresholds:", paste(missing_thresholds, collapse = ", "), "\n")
    
    x_max <- max(model_result$x_values)
    current_max_features <- max(model_result$y_values)
    
    for (missing_thresh in missing_thresholds) {
      # Conservative extrapolation for very efficient curves that don't reach low thresholds
      if (missing_thresh == 10) {
        fallback_depth <- x_max * 2    # Double current sequencing
      } else if (missing_thresh == 1) {
        fallback_depth <- x_max * 5    # 5x current sequencing
      } else if (missing_thresh == 0.1) {
        fallback_depth <- x_max * 15   # 15x current sequencing
      } else {
        # Interpolate for other thresholds
        fallback_depth <- x_max * (20 / missing_thresh)  # Inverse relationship
      }
      
      # Predict features at fallback depth
      tryCatch({
        fallback_features <- predict(model_result$fit_object, newdata = data.frame(x = fallback_depth))
        fallback_features <- as.numeric(fallback_features)
        
        if (!is.finite(fallback_features) || fallback_features <= 0) {
          # Ultimate fallback: linear extrapolation
          growth_rate <- current_max_features / x_max
          fallback_features <- growth_rate * fallback_depth * 0.7  # Conservative estimate
        }
        
        threshold_depths <- rbind(threshold_depths, data.frame(
          threshold = missing_thresh,
          depth = fallback_depth,
          features = fallback_features,
          fold_increase_from_current = fallback_depth / x_max
        ))
        
        cat("    ", missing_thresh, "f/M: fallback depth =", round(fallback_depth), "reads, features =", round(fallback_features), "\n")
        
      }, error = function(e) {
        cat("    ", missing_thresh, "f/M: even fallback failed\n")
      })
    }
  }
  
  return(threshold_depths)
}

# Bootstrap confidence intervals for threshold depths
bootstrap_threshold_uncertainty <- function(curve_data, model_type, threshold, n_boot = N_BOOTSTRAP) {
  n_points <- nrow(curve_data)
  
  # Store bootstrap results
  threshold_depths <- numeric(n_boot)
  threshold_features <- numeric(n_boot)
  converged <- logical(n_boot)
  
  cat("  Bootstrapping", threshold, "f/M threshold (", n_boot, "iterations)...\n")
  
  # Use parallel processing for efficiency
  n_cores <- min(detectCores() - 1, 8)  # Leave one core free, max 8
  
  # Sequential processing (parallel code optional)
  for (i in 1:n_boot) {
    # Resample with replacement
    boot_indices <- sample(n_points, replace = TRUE)
    boot_data <- curve_data[boot_indices, ]
    
    # Refit model
    boot_model <- fit_model_robust(
      x = boot_data$total_reads_unified,
      y = boot_data$featureNum,
      model_type = model_type
    )
    
    if (boot_model$convergence) {
      # Calculate threshold for this bootstrap sample
      thresh_result <- find_threshold_depths(boot_model, threshold)
      if (nrow(thresh_result) > 0) {
        threshold_depths[i] <- thresh_result$depth[1]
        threshold_features[i] <- thresh_result$features[1]
        converged[i] <- TRUE
      }
    }
  }
  
  # Calculate statistics
  valid_depths <- threshold_depths[converged]
  valid_features <- threshold_features[converged]
  
  if (length(valid_depths) < 10) {
    cat("    WARNING: Only", length(valid_depths), "bootstrap samples converged\n")
  }
  
  # Calculate confidence intervals
  depth_quantiles <- quantile(valid_depths, probs = c(0.025, 0.5, 0.975), na.rm = TRUE)
  features_quantiles <- quantile(valid_features, probs = c(0.025, 0.5, 0.975), na.rm = TRUE)
  
  result <- list(
    threshold = threshold,
    n_converged = sum(converged),
    convergence_rate = mean(converged),
    depth_median = depth_quantiles[2],
    depth_ci = depth_quantiles[c(1, 3)],
    depth_cv = sd(valid_depths, na.rm = TRUE) / mean(valid_depths, na.rm = TRUE),
    features_median = features_quantiles[2],
    features_ci = features_quantiles[c(1, 3)],
    features_cv = sd(valid_features, na.rm = TRUE) / mean(valid_features, na.rm = TRUE),
    all_depths = valid_depths,
    all_features = valid_features
  )
  
  cat("    Depth:", round(result$depth_median/1e6, 1), "M reads",
      "[95% CI:", round(result$depth_ci[1]/1e6, 1), "-", 
      round(result$depth_ci[2]/1e6, 1), "M] | CV:", 
      round(result$depth_cv * 100, 1), "%\n")
  
  return(result)
}

## Model Fitting Execution with Bootstrap -------
cat("\n=== FITTING MODEL SUITE WITH AIC SELECTION AND BOOTSTRAP UNCERTAINTY ===\n")

# Enhanced fitting with bootstrap
fitted_models_with_uncertainty <- rarefaction_data %>%
  group_by(curve_id) %>%
  group_modify(~ {
    current_curve_id <- .y$curve_id
    data_with_id <- .x %>% mutate(curve_id = current_curve_id)
    
    # Fit primary model
    result <- fit_best_model_aic(data_with_id)
    
    # Run bootstrap for all thresholds if model converged
    bootstrap_results <- list()
    if (result$convergence) {
      for (thresh in MARGINAL_THRESHOLDS) {
        cat("\n  Bootstrap analysis for", current_curve_id, "at", thresh, "f/M:\n")
        boot_result <- bootstrap_threshold_uncertainty(
          curve_data = data_with_id,
          model_type = result$model,
          threshold = thresh,
          n_boot = N_BOOTSTRAP
        )
        bootstrap_results[[as.character(thresh)]] <- boot_result
      }
    }
    
    tibble(
      model_type = result$model,
      r_squared = result$r_squared,
      aic = result$aic %||% NA,
      convergence = result$convergence,
      model_uncertainty = result$model_uncertainty %||% "unknown",
      fitted_model = list(result),
      bootstrap_results = list(bootstrap_results)
    )
  })

# Also maintain the original fitted_models for compatibility
fitted_models <- fitted_models_with_uncertainty %>%
  select(-bootstrap_results)

# Save enhanced fitted models
fitted_models_file <- file.path(stats_dir, "fitted_models_with_bootstrap.rds")
saveRDS(fitted_models_with_uncertainty, fitted_models_file)

# Save original format for compatibility
fitted_models_original_file <- file.path(stats_dir, "fitted_models_marginal_returns.rds")
saveRDS(fitted_models, fitted_models_original_file)

cat("Model fitting complete. Results saved to:", fitted_models_file, "\n")
cat("Model selection summary:\n")
print(table(fitted_models$model_type, useNA = "ifany"))

# Debug: Check which models failed to converge
failed_models <- fitted_models %>% filter(!convergence)
if(nrow(failed_models) > 0) {
  cat("\nFAILED TO CONVERGE:\n")
  cat(paste("-", failed_models$curve_id, collapse = "\n"), "\n")
}

converged_models <- fitted_models %>% filter(convergence)
cat("\nSuccessfully fitted models:\n")
cat(paste("-", converged_models$curve_id, collapse = "\n"), "\n")

## Marginal Returns Analysis -------
cat("\n=== CALCULATING MARGINAL RETURNS FOR ALL PROTOCOLS ===\n")

# Calculate threshold depths for all curves
marginal_returns_results <- data.frame()

for (i in 1:nrow(fitted_models)) {
  if (!fitted_models$fitted_model[[i]]$convergence) {
    cat("Skipping", fitted_models$curve_id[i], "- no convergence\n")
    next
  }
  
  curve_id <- fitted_models$curve_id[i]
  model_result <- fitted_models$fitted_model[[i]]
  
  cat("Processing marginal returns for:", curve_id, "\n")
  
  # Extract metadata
  technology <- str_extract(curve_id, "^[^_]+")
  sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
  feature_type <- str_extract(curve_id, "[^_]+$")
  
  # Debug: Check current curve data
  current_max_features <- max(model_result$y_values)
  current_max_depth <- max(model_result$x_values)
  cat("  Current max: ", current_max_features, "features at", current_max_depth, "reads\n")
  
  # Get threshold depths
  thresh_depths <- find_threshold_depths(model_result, MARGINAL_THRESHOLDS)
  
  # Debug: Check if results make sense (0.1 f/M should have more features than 1 f/M)
  if(nrow(thresh_depths) > 1) {
    thresh_10 <- thresh_depths[thresh_depths$threshold == 10, "features"]
    thresh_1 <- thresh_depths[thresh_depths$threshold == 1, "features"] 
    thresh_0.1 <- thresh_depths[thresh_depths$threshold == 0.1, "features"]
    
    if(length(thresh_10) > 0 && length(thresh_1) > 0 && thresh_10 > thresh_1) {
      cat("  WARNING: 10 f/M has more features than 1 f/M - this is wrong!\n")
    }
    if(length(thresh_1) > 0 && length(thresh_0.1) > 0 && thresh_1 > thresh_0.1) {
      cat("  WARNING: 1 f/M has more features than 0.1 f/M - this is wrong!\n")
    }
  }
  
  if (nrow(thresh_depths) > 0) {
    thresh_depths$curve_id <- curve_id
    thresh_depths$technology <- technology
    thresh_depths$sample_type <- sample_type
    thresh_depths$feature_type <- feature_type
    thresh_depths$model_type <- model_result$model
    thresh_depths$current_max_depth <- max(model_result$x_values)
    thresh_depths$current_max_features <- max(model_result$y_values)
    
    # Add bootstrap statistics to threshold depths
    bootstrap_results <- fitted_models_with_uncertainty$bootstrap_results[[i]]
    for (j in 1:nrow(thresh_depths)) {
      thresh <- thresh_depths$threshold[j]
      boot_stats <- bootstrap_results[[as.character(thresh)]]
      
      if (!is.null(boot_stats)) {
        thresh_depths$depth_ci_lower[j] <- boot_stats$depth_ci[1]
        thresh_depths$depth_ci_upper[j] <- boot_stats$depth_ci[2]
        thresh_depths$depth_cv[j] <- boot_stats$depth_cv
        thresh_depths$features_ci_lower[j] <- boot_stats$features_ci[1]
        thresh_depths$features_ci_upper[j] <- boot_stats$features_ci[2]
        thresh_depths$bootstrap_convergence_rate[j] <- boot_stats$convergence_rate
        thresh_depths$extrapolation_factor[j] <- thresh_depths$depth[j] / max(model_result$x_values)
        
        # Categorize model stability (based on CV alone)
        thresh_depths$model_stability[j] <- case_when(
          thresh_depths$depth_cv[j] < 0.05 ~ "High",
          thresh_depths$depth_cv[j] < 0.15 ~ "Moderate",
          thresh_depths$depth_cv[j] < 0.30 ~ "Low",
          TRUE ~ "Very Low"
        )
      } else {
        # Fill with defaults for missing bootstrap results
        thresh_depths$depth_ci_lower[j] <- NA
        thresh_depths$depth_ci_upper[j] <- NA
        thresh_depths$depth_cv[j] <- NA
        thresh_depths$features_ci_lower[j] <- NA
        thresh_depths$features_ci_upper[j] <- NA
        thresh_depths$bootstrap_convergence_rate[j] <- NA
        thresh_depths$extrapolation_factor[j] <- thresh_depths$depth[j] / max(model_result$x_values)
        thresh_depths$model_stability[j] <- "Unknown"
      }
    }
    
    marginal_returns_results <- rbind(marginal_returns_results, thresh_depths)
  }
}

# Save marginal returns results
marginal_returns_file <- file.path(stats_dir, "marginal_returns_analysis.csv")
write.csv(marginal_returns_results, marginal_returns_file, row.names = FALSE)

cat("Marginal returns analysis saved to:", marginal_returns_file, "\n")

# Debug: Check what we got for marginal returns
cat("Marginal returns calculated for", length(unique(marginal_returns_results$curve_id)), "curves:\n")
for(curve in sort(unique(marginal_returns_results$curve_id))) {
  thresholds_found <- marginal_returns_results %>% 
    filter(curve_id == curve) %>% 
    pull(threshold) %>% 
    sort()
  cat("-", curve, ":", paste(thresholds_found, collapse = ", "), "\n")
}

# Debug: Check specifically for missing thresholds
cat("\nDEBUGGING MISSING THRESHOLDS:\n")
expected_curves <- c(
  "ONT_Bulk_Genes Discovery", "ONT_Bulk_Isoforms Discovery",
  "ONT_SC_Genes Discovery", "ONT_SC_Isoforms Discovery", 
  "ONT_SN_Genes Discovery", "ONT_SN_Isoforms Discovery",
  "PacBio_Bulk_Genes Discovery", "PacBio_Bulk_Isoforms Discovery",
  "PacBio_SC_Genes Discovery", "PacBio_SC_Isoforms Discovery",
  "PacBio_SN_Genes Discovery", "PacBio_SN_Isoforms Discovery"
)

for(thresh in MARGINAL_THRESHOLDS) {
  missing_for_thresh <- setdiff(expected_curves, 
                               marginal_returns_results[marginal_returns_results$threshold == thresh, "curve_id"])
  if(length(missing_for_thresh) > 0) {
    cat("Missing", thresh, "f/M threshold:\n")
    cat(paste("-", missing_for_thresh, collapse = "\n"), "\n")
  }
}

## Enhanced Visualizations -------
cat("\n=== CREATING VISUALIZATIONS ===\n")

# Create fitted curve data
create_fitted_curves <- function(fitted_models, rarefaction_data) {
  fitted_curve_data <- list()
  
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    model_result <- fitted_models$fitted_model[[i]]
    
    if (!model_result$convergence) next
    
    # Get original data range
    orig_data <- rarefaction_data %>% filter(curve_id == !!curve_id)
    x_range <- range(orig_data$total_reads_unified, na.rm = TRUE)
    x_smooth <- seq(from = x_range[1], to = x_range[2], length.out = 100)
    
    # Generate predictions
    y_smooth <- predict(model_result$fit_object, newdata = data.frame(x = x_smooth))
    
    curve_data <- tibble(
      curve_id = curve_id,
      total_reads_unified = x_smooth,
      featureNum_fitted = y_smooth,
      model_type = model_result$model,
      technology = str_extract(curve_id, "^[^_]+"),
      sample_type = str_extract(curve_id, "(?<=_)[^_]+(?=_)"),
      feature_type = str_extract(curve_id, "[^_]+$")
    )
    
    fitted_curve_data[[i]] <- curve_data
  }
  
  return(bind_rows(fitted_curve_data))
}

fitted_curves <- create_fitted_curves(fitted_models, rarefaction_data)

# Save plots directory
plots_dir <- file.path(stats_dir, "plots")
if (!dir.exists(plots_dir)) dir.create(plots_dir, recursive = TRUE)

# 1. Individual Model Fit Plots with R-squared (keep existing style)
cat("\n=== CREATING INDIVIDUAL MODEL FIT PLOTS ===\n")

create_individual_fit_plots <- function(fitted_models, rarefaction_data, fitted_curves) {
  plot_list <- list()
  
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    model_result <- fitted_models$fitted_model[[i]]
    
    if (!model_result$convergence) {
      cat("Skipping", curve_id, "- model did not converge\n")
      next
    }
    
    # Get original data for this curve
    orig_data <- rarefaction_data %>% filter(curve_id == !!curve_id)
    
    # Get fitted curve data for this curve
    fitted_data <- fitted_curves %>% filter(curve_id == !!curve_id)
    
    # Extract metadata
    technology <- str_extract(curve_id, "^[^_]+")
    sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
    feature_type <- str_extract(curve_id, "[^_]+$")
    
    # Create title with model info
    model_type_display <- case_when(
      model_result$model == "michaelis_menten" ~ "Michaelis-Menten",
      model_result$model == "asymptotic_exp" ~ "Asymptotic Exponential",
      model_result$model == "power_law" ~ "Power Law",
      model_result$model == "logarithmic" ~ "Logarithmic",
      model_result$model == "shifted_logarithmic" ~ "Shifted Logarithmic",
      model_result$model == "hill" ~ "Hill Equation",
      TRUE ~ model_result$model
    )
    
    plot_title <- paste0(technology, " ", 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         ), " ", 
                         feature_type)
    
    subtitle <- paste0("Model: ", model_type_display, 
                       " | R² = ", round(model_result$r_squared, 3))
    
    # Create individual plot
    p <- ggplot() +
      # Add fitted curve
      geom_line(data = fitted_data, 
                aes(x = total_reads_unified, y = featureNum_fitted),
                color = color_map[technology], 
                linetype = linetype_map[sample_type],
                linewidth = 1.5, alpha = 0.8) +
      # Add original data points
      geom_point(data = orig_data,
                 aes(x = total_reads_unified, y = featureNum),
                 color = color_map[technology], 
                 shape = shape_map[sample_type],
                 size = 3, alpha = 0.8) +
      labs(title = plot_title,
           subtitle = subtitle,
           x = "Total Reads",
           y = "Number of Features Detected") +
      theme_minimal() +
      theme(
        text = element_text(size = 10),
        plot.title = element_text(size = 11, face = "bold"),
        plot.subtitle = element_text(size = 9, color = "gray50"),
        axis.text.x = element_text(angle = 45, hjust = 1)
      ) +
      scale_x_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 5)) +
      scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 5))
    
    plot_list[[curve_id]] <- p
  }
  
  return(plot_list)
}

# Function to create individual plots with reads per cell x-axis
create_individual_fit_plots_reads_per_cell <- function(fitted_models, rarefaction_data, fitted_curves) {
  plot_list <- list()
  
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    model_result <- fitted_models$fitted_model[[i]]
    
    if (!model_result$convergence) {
      cat("Skipping", curve_id, "- model did not converge\n")
      next
    }
    
    # Get original data for this curve
    orig_data <- rarefaction_data %>% filter(curve_id == !!curve_id)
    
    # Get fitted curve data for this curve
    fitted_data <- fitted_curves %>% filter(curve_id == !!curve_id)
    
    # Transform x-axis to reads per cell
    orig_data_transformed <- orig_data %>%
      mutate(reads_per_cell = reads_per_cell_converter(total_reads_unified))
    
    fitted_data_transformed <- fitted_data %>%
      mutate(reads_per_cell = reads_per_cell_converter(total_reads_unified))
    
    # Extract metadata
    technology <- str_extract(curve_id, "^[^_]+")
    sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
    feature_type <- str_extract(curve_id, "[^_]+$")
    
    # Create title with model info
    model_type_display <- case_when(
      model_result$model == "michaelis_menten" ~ "Michaelis-Menten",
      model_result$model == "asymptotic_exp" ~ "Asymptotic Exponential",
      model_result$model == "power_law" ~ "Power Law",
      model_result$model == "logarithmic" ~ "Logarithmic",
      model_result$model == "shifted_logarithmic" ~ "Shifted Logarithmic",
      model_result$model == "hill" ~ "Hill Equation",
      TRUE ~ model_result$model
    )
    
    plot_title <- paste0(technology, " ", 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         ), " ", 
                         feature_type)
    
    subtitle <- paste0("Model: ", model_type_display, 
                       " | R² = ", round(model_result$r_squared, 3))
    
    # Create individual plot with reads per cell x-axis
    p <- ggplot() +
      # Add fitted curve
      geom_line(data = fitted_data_transformed, 
                aes(x = reads_per_cell, y = featureNum_fitted),
                color = color_map[technology], 
                linetype = linetype_map[sample_type],
                linewidth = 1.5, alpha = 0.8) +
      # Add original data points
      geom_point(data = orig_data_transformed,
                 aes(x = reads_per_cell, y = featureNum),
                 color = color_map[technology], 
                 shape = shape_map[sample_type],
                 size = 3, alpha = 0.8) +
      labs(title = plot_title,
           subtitle = subtitle,
           x = "Estimated Median Reads per Cell",
           y = "Number of Features Detected") +
      theme_minimal() +
      theme(
        text = element_text(size = 10),
        plot.title = element_text(size = 11, face = "bold"),
        plot.subtitle = element_text(size = 9, color = "gray50"),
        axis.text.x = element_text(angle = 45, hjust = 1)
      ) +
      scale_x_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 5)) +
      scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 5))
    
    plot_list[[curve_id]] <- p
  }
  
  return(plot_list)
}

# Create individual plots (original with total reads)
individual_plots <- create_individual_fit_plots(fitted_models, rarefaction_data, fitted_curves)

# Create individual plots with reads per cell x-axis
individual_plots_reads_per_cell <- create_individual_fit_plots_reads_per_cell(fitted_models, rarefaction_data, fitted_curves)

# Save individual model fit plots (original)
if (length(individual_plots) > 0) {
  # Calculate grid dimensions
  n_plots <- length(individual_plots)
  n_cols <- min(3, n_plots)  # Max 3 columns
  n_rows <- ceiling(n_plots / n_cols)
  
  # Combine all plots
  combined_individual_plots <- wrap_plots(individual_plots, ncol = n_cols)
  
  # Save individual model fit plots
  individual_fits_pdf <- file.path(plots_dir, "individual_model_fits_with_rsquared.pdf")
  ggsave(individual_fits_pdf, plot = combined_individual_plots, 
         width = n_cols * 5, height = n_rows * 4, device = "pdf")
  
  cat("Individual model fit plots saved to:", individual_fits_pdf, "\n")
}

# Save individual model fit plots (reads per cell version)
if (length(individual_plots_reads_per_cell) > 0) {
  # Calculate grid dimensions
  n_plots <- length(individual_plots_reads_per_cell)
  n_cols <- min(3, n_plots)  # Max 3 columns
  n_rows <- ceiling(n_plots / n_cols)
  
  # Combine all plots
  combined_individual_plots_rpc <- wrap_plots(individual_plots_reads_per_cell, ncol = n_cols)
  
  # Save individual model fit plots with reads per cell x-axis
  individual_fits_rpc_pdf <- file.path(plots_dir, "individual_model_fits_reads_per_cell.pdf")
  ggsave(individual_fits_rpc_pdf, plot = combined_individual_plots_rpc, 
         width = n_cols * 5, height = n_rows * 4, device = "pdf")
  
  cat("Individual model fit plots (reads per cell) saved to:", individual_fits_rpc_pdf, "\n")
}

# 2. Extended Saturation Curves with Thresholds
cat("\n=== CREATING EXTENDED SATURATION CURVES ===\n")

create_extended_saturation_plots <- function(fitted_models, rarefaction_data, marginal_returns_results) {
  plot_list <- list()
  
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    model_result <- fitted_models$fitted_model[[i]]
    
    if (!model_result$convergence) next
    
    # Get threshold info for this curve
    thresh_info <- marginal_returns_results %>% filter(curve_id == !!curve_id)
    if (nrow(thresh_info) == 0) next
    
    # Get original data
    orig_data <- rarefaction_data %>% filter(curve_id == !!curve_id)
    
    # Extract metadata
    technology <- str_extract(curve_id, "^[^_]+")
    sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
    feature_type <- str_extract(curve_id, "[^_]+$")
    
    # Determine extension range - extend to 1 f/M threshold (as user wants)
    max_x_orig <- max(orig_data$total_reads_unified, na.rm = TRUE)
    
    # Get 1 f/M threshold depth, fall back to conservative extension if not available
    thresh_1fm <- thresh_info %>% filter(threshold == 1)
    if (nrow(thresh_1fm) > 0) {
      extend_to_x <- thresh_1fm$depth[1] * 1.3  # Extend 30% past 1 f/M threshold
    } else {
      extend_to_x <- max_x_orig * 5  # Conservative fallback
    }
    
    # Create extended x range
    x_extended <- seq(from = min(orig_data$total_reads_unified), to = extend_to_x, length.out = 300)
    
    # Generate extended predictions
    y_extended <- predict(model_result$fit_object, newdata = data.frame(x = x_extended))
    
    # Create extended curve data with reads per cell conversion
    extended_data <- tibble(
      total_reads_unified = x_extended,
      featureNum_fitted = y_extended,
      reads_per_cell = reads_per_cell_converter(x_extended)
    )
    
    # Also convert original data
    orig_data_converted <- orig_data %>%
      mutate(reads_per_cell = reads_per_cell_converter(total_reads_unified))
    
    # Create title
    model_type_display <- case_when(
      model_result$model == "michaelis_menten" ~ "Michaelis-Menten",
      model_result$model == "asymptotic_exp" ~ "Asymptotic Exponential",
      model_result$model == "power_law" ~ "Power Law",
      model_result$model == "logarithmic" ~ "Logarithmic",
      model_result$model == "shifted_logarithmic" ~ "Shifted Logarithmic",
      model_result$model == "hill" ~ "Hill Equation",
      TRUE ~ model_result$model
    )
    
    plot_title <- paste0(technology, " ", 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         ), " ", 
                         feature_type)
    
    # Create subtitle based on whether 1 f/M threshold was found
    if (nrow(thresh_1fm) > 0) {
      subtitle <- paste0(model_type_display, " | Extended to 1 f/M Threshold")
      extension_label <- "Extended to\n1 f/M"
    } else {
      fold_increase <- round(extend_to_x / max_x_orig, 1)
      subtitle <- paste0(model_type_display, " | Extended ", fold_increase, "x (no 1 f/M threshold)")
      extension_label <- paste0("Extended\n", fold_increase, "x")
    }
    
    # Create the plot
    p <- ggplot() +
      # Add extended fitted curve
      geom_line(data = extended_data, 
                aes(x = reads_per_cell, y = featureNum_fitted),
                color = color_map[technology], 
                linetype = linetype_map[sample_type],
                linewidth = 1.2, alpha = 0.7) +
      # Add original data points
      geom_point(data = orig_data_converted,
                 aes(x = reads_per_cell, y = featureNum),
                 color = color_map[technology], 
                 shape = shape_map[sample_type],
                 size = 2.5, alpha = 0.9) +
      # Add vertical line at current max depth (converted)
      geom_vline(xintercept = reads_per_cell_converter(max_x_orig), linetype = "dashed", alpha = 0.5, color = "gray50") +
      # Add threshold lines if 1 f/M threshold exists (converted)
      {if (nrow(thresh_1fm) > 0) {
        list(
          geom_vline(xintercept = reads_per_cell_converter(thresh_1fm$depth[1]), linetype = "dotted", alpha = 0.7, color = "darkgreen"),
          geom_hline(yintercept = thresh_1fm$features[1], linetype = "dotted", alpha = 0.5, color = "darkgreen")
        )
      }} +
      # Annotate regions (converted positions)
      annotate("text", x = reads_per_cell_converter(max_x_orig) * 0.5, y = max(extended_data$featureNum_fitted) * 0.95, 
               label = "Current\nData", size = 3, alpha = 0.7, hjust = 0.5) +
      annotate("text", x = reads_per_cell_converter(extend_to_x) * 0.8, y = max(extended_data$featureNum_fitted) * 0.95, 
               label = extension_label, size = 3, alpha = 0.7, hjust = 0.5) +
      labs(title = plot_title,
           subtitle = subtitle,
           x = "Estimated Median Reads per Cell",
           y = "Number of Features Detected") +
      theme_minimal() +
      theme(
        text = element_text(size = 10),
        plot.title = element_text(size = 11, face = "bold"),
        plot.subtitle = element_text(size = 9, color = "gray50"),
        axis.text.x = element_text(angle = 45, hjust = 1)
      ) +
      scale_x_continuous(
        labels = scales::comma, 
        breaks = scales::pretty_breaks(n = 5)
      ) +
      scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 5))
    
    plot_list[[curve_id]] <- p
  }
  
  return(plot_list)
}

# Create extended saturation plots
extended_plots <- create_extended_saturation_plots(fitted_models, rarefaction_data, marginal_returns_results)

# Save extended saturation plots
if (length(extended_plots) > 0) {
  # Create technology-grouped grid layouts first
  create_technology_grouped_layouts <- function(extended_plots) {
    # Group plots by technology
    ont_plots <- list()
    pacbio_plots <- list()
    
    for (plot_name in names(extended_plots)) {
      if (grepl("^ONT", plot_name)) {
        ont_plots[[plot_name]] <- extended_plots[[plot_name]]
      } else if (grepl("^PacBio", plot_name)) {
        pacbio_plots[[plot_name]] <- extended_plots[[plot_name]]
      }
    }
    
    # Create grid layouts
    grouped_layouts <- list()
    
    # ONT Technology Grid
    if (length(ont_plots) > 0) {
      # Create ordered arrangement: Top row (Genes), Bottom row (Isoforms)
      # Order within each row: Bulk, Single-Cell, Single-Nucleus
      sample_order <- c("Bulk", "SC", "SN")
      
      # Find plots for specific positions
      ordered_ont_plots <- list()
      
      # Top row: Genes (Bulk, SC, SN)
      for (sample_type in sample_order) {
        pattern <- paste0("ONT_", sample_type, "_.*Genes Discovery")
        matching_plot <- ont_plots[grepl(pattern, names(ont_plots))]
        if (length(matching_plot) > 0) {
          ordered_ont_plots <- c(ordered_ont_plots, matching_plot)
        }
      }
      
      # Bottom row: Isoforms (Bulk, SC, SN)  
      for (sample_type in sample_order) {
        pattern <- paste0("ONT_", sample_type, "_.*Isoforms Discovery")
        matching_plot <- ont_plots[grepl(pattern, names(ont_plots))]
        if (length(matching_plot) > 0) {
          ordered_ont_plots <- c(ordered_ont_plots, matching_plot)
        }
      }
      
      # Modify plots for grid display (remove individual titles, smaller text)
      ont_grid_plots <- lapply(ordered_ont_plots, function(p) {
        p + 
          theme(
            plot.title = element_text(size = 10, face = "bold"),
            plot.subtitle = element_text(size = 8, color = "gray50"),
            axis.title = element_text(size = 9),
            axis.text = element_text(size = 8),
            legend.position = "none"  # Remove individual legends
          )
      })
      
      # Create 2x3 grid layout (2 rows, 3 columns)
      ont_grid <- wrap_plots(ont_grid_plots, ncol = 3, nrow = 2, byrow = TRUE) +
        plot_annotation(
          title = "ONT Extended Saturation Curves",
          subtitle = "Top row: Gene Discovery | Bottom row: Isoform Discovery | Left to right: Bulk, Single-Cell, Single-Nucleus",
          theme = theme(
            plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            plot.subtitle = element_text(size = 10, color = "gray40", hjust = 0.5)
          )
        )
      
      grouped_layouts[["ONT_Grid"]] <- ont_grid
    }
    
    # PacBio Technology Grid  
    if (length(pacbio_plots) > 0) {
      # Create ordered arrangement: Top row (Genes), Bottom row (Isoforms)
      # Order within each row: Bulk, Single-Cell, Single-Nucleus
      sample_order <- c("Bulk", "SC", "SN")
      
      # Find plots for specific positions
      ordered_pacbio_plots <- list()
      
      # Top row: Genes (Bulk, SC, SN)
      for (sample_type in sample_order) {
        pattern <- paste0("PacBio_", sample_type, "_.*Genes Discovery")
        matching_plot <- pacbio_plots[grepl(pattern, names(pacbio_plots))]
        if (length(matching_plot) > 0) {
          ordered_pacbio_plots <- c(ordered_pacbio_plots, matching_plot)
        }
      }
      
      # Bottom row: Isoforms (Bulk, SC, SN)
      for (sample_type in sample_order) {
        pattern <- paste0("PacBio_", sample_type, "_.*Isoforms Discovery")
        matching_plot <- pacbio_plots[grepl(pattern, names(pacbio_plots))]
        if (length(matching_plot) > 0) {
          ordered_pacbio_plots <- c(ordered_pacbio_plots, matching_plot)
        }
      }
      
      # Modify plots for grid display (remove individual titles, smaller text)
      pacbio_grid_plots <- lapply(ordered_pacbio_plots, function(p) {
        p + 
          theme(
            plot.title = element_text(size = 10, face = "bold"),
            plot.subtitle = element_text(size = 8, color = "gray50"),
            axis.title = element_text(size = 9),
            axis.text = element_text(size = 8),
            legend.position = "none"  # Remove individual legends
          )
      })
      
      # Create 2x3 grid layout (2 rows, 3 columns)
      pacbio_grid <- wrap_plots(pacbio_grid_plots, ncol = 3, nrow = 2, byrow = TRUE) +
        plot_annotation(
          title = "PacBio Extended Saturation Curves", 
          subtitle = "Top row: Gene Discovery | Bottom row: Isoform Discovery | Left to right: Bulk, Single-Cell, Single-Nucleus",
          theme = theme(
            plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            plot.subtitle = element_text(size = 10, color = "gray40", hjust = 0.5)
          )
        )
      
      grouped_layouts[["PacBio_Grid"]] <- pacbio_grid
    }
    
    return(grouped_layouts)
  }
  
  # Create the grouped layouts
  technology_grids <- create_technology_grouped_layouts(extended_plots)
  
  # Save as multi-page PDF with technology grids first, then individual plots
  extended_pdf <- file.path(plots_dir, "extended_saturation_curves.pdf")
  
  pdf(extended_pdf, width = 12, height = 10)  # Wider for grid layouts
  
  # First, add technology-grouped grid layouts
  if (length(technology_grids) > 0) {
    cat("Adding technology-grouped grid layouts to extended curves PDF...\n")
    for (grid_name in names(technology_grids)) {
      cat("  Adding", grid_name, "\n")
      print(technology_grids[[grid_name]])
    }
  }
  
  # Then add individual plots
  cat("Adding individual extended saturation curves...\n")
  for (plot_name in names(extended_plots)) {
    print(extended_plots[[plot_name]])
  }
  
  dev.off()
  
  cat("Extended saturation curves saved to:", extended_pdf, "\n")
  if (length(technology_grids) > 0) {
    cat("- Includes technology-grouped grid layouts at the start\n")
    for (grid_name in names(technology_grids)) {
      cat("  *", grid_name, "\n")
    }
  }
}

# 2b. Combined Extended Saturation Curves by Feature Type  
cat("\n=== CREATING COMBINED EXTENDED SATURATION CURVES ===\n")

create_combined_extended_saturation_plots <- function(fitted_models, rarefaction_data, marginal_returns_results) {
  plot_list <- list()
  
  # Group by feature type  
  feature_types <- c("Genes Discovery", "Isoforms Discovery")
  
  for (ft in feature_types) {
    cat("Processing combined plots for:", ft, "\n")
    
    # Get all curves for this feature type
    curves_for_ft <- fitted_models %>% 
      filter(str_detect(curve_id, paste0(ft, "$")), convergence == TRUE)
    
    cat("Found", nrow(curves_for_ft), "converged curves for", ft, "\n")
    
    if (nrow(curves_for_ft) == 0) {
      cat("No converged curves found for", ft, "\n")
      next
    }
    
        # Find the maximum 1 f/M threshold depth across all curves for this feature type (as user wants)
    thresh_1fm_for_ft <- marginal_returns_results %>%
      filter(str_detect(curve_id, paste0(ft, "$")), threshold == 1)
    
    if (nrow(thresh_1fm_for_ft) == 0) {
      cat("No 1 f/M thresholds found for", ft, "- using conservative extension\n")
      # Fall back to conservative extension
      first_curve_data <- rarefaction_data %>% 
        filter(str_detect(curve_id, paste0(ft, "$"))) %>%
        slice(1)
      max_extend_to <- max(first_curve_data$total_reads_unified, na.rm = TRUE) * 5
    } else {
      # Use the maximum 1 f/M threshold depth across all protocols for this feature type
      max_extend_to <- max(thresh_1fm_for_ft$depth, na.rm = TRUE) * 1.3  # Extend 30% past furthest 1 f/M
      cat("  Extending to", scales::comma(max_extend_to), "reads for", ft, "(30% past furthest 1 f/M threshold)\n")
    }
    
    # Create extended curves for each protocol
    all_extended_data <- data.frame()
    all_original_data <- data.frame()
    
    for (i in 1:nrow(curves_for_ft)) {
      curve_id <- curves_for_ft$curve_id[i]
      model_result <- curves_for_ft$fitted_model[[i]]
      
      # Extract metadata
      technology <- str_extract(curve_id, "^[^_]+")
      sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
      
      # Get original data
      orig_data <- rarefaction_data %>% filter(curve_id == !!curve_id)
      orig_data$technology <- technology
      orig_data$sample_type <- sample_type
      orig_data$protocol <- paste(technology, sample_type)
      
      # Create extended x range
      min_x <- min(orig_data$total_reads_unified, na.rm = TRUE)
      x_extended <- seq(from = min_x, to = max_extend_to, length.out = 500)
      
      # Generate extended predictions
      y_extended <- predict(model_result$fit_object, newdata = data.frame(x = x_extended))
      
      # Create extended curve data
      extended_data <- data.frame(
        curve_id = curve_id,
        total_reads_unified = x_extended,
        featureNum_fitted = y_extended,
        technology = technology,
        sample_type = sample_type,
        protocol = paste(technology, sample_type)
      )
      
      all_extended_data <- rbind(all_extended_data, extended_data)
      all_original_data <- rbind(all_original_data, orig_data)
    }
    
    # Create the combined plot
    feature_name <- str_remove(ft, " Discovery")
    
    # Create subtitle based on whether 1 f/M thresholds were found
    if (nrow(thresh_1fm_for_ft) > 0) {
      subtitle_text <- "Extended to 1 f/M threshold (furthest curve)"
    } else {
      max_current_depth <- max(all_original_data$total_reads_unified, na.rm = TRUE)
      fold_increase <- round(max_extend_to / max_current_depth, 1)
      subtitle_text <- paste("Extended", fold_increase, "x current depth (no 1 f/M thresholds found)")
    }
    
    p <- ggplot() +
      # Add extended fitted curves
      geom_line(data = all_extended_data, 
                aes(x = total_reads_unified, y = featureNum_fitted,
                    color = technology, linetype = sample_type, group = curve_id),
                linewidth = 1.2, alpha = 0.8) +
      # Add original data points
      geom_point(data = all_original_data,
                 aes(x = total_reads_unified, y = featureNum,
                     color = technology, shape = sample_type),
                 size = 2, alpha = 0.7) +
      # Color and line mappings
      scale_color_manual(values = color_map, name = "Technology") +
      scale_linetype_manual(values = linetype_map, name = "Sample Type",
                           labels = c("Bulk" = "Bulk", "SC" = "Single-Cell", "SN" = "Single-Nucleus")) +
      scale_shape_manual(values = shape_map, name = "Sample Type",
                        labels = c("Bulk" = "Bulk", "SC" = "Single-Cell", "SN" = "Single-Nucleus")) +
      # Axis formatting
      scale_x_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 6)) +
      scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 6)) +
      # Labels and theme
      labs(title = paste("Extended Saturation Curves:", feature_name),
           subtitle = subtitle_text,
           x = "Total Reads",
           y = paste("Number of", feature_name, "Detected")) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10, color = "gray50"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.box = "horizontal",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        panel.grid.minor = element_blank()
      ) +
      guides(
        color = guide_legend(title = "Technology", override.aes = list(linewidth = 2)),
        linetype = guide_legend(title = "Sample Type", override.aes = list(linewidth = 2)),
        shape = guide_legend(title = "Sample Type")
      )
    
        # Create unique plot name
    plot_name <- paste0(ft, "_Extended")
    plot_list[[plot_name]] <- p
    
    # Create technology-faceted version of the same plot
    p_faceted <- ggplot() +
      # Add extended fitted curves
      geom_line(data = all_extended_data, 
                aes(x = total_reads_unified, y = featureNum_fitted,
                    color = technology, linetype = sample_type, group = curve_id),
                linewidth = 1.2, alpha = 0.8) +
      # Add original data points
      geom_point(data = all_original_data,
                 aes(x = total_reads_unified, y = featureNum,
                     color = technology, shape = sample_type),
                 size = 2, alpha = 0.7) +
      # Facet by technology
      facet_wrap(~technology, ncol = 2, scales = "free_y") +
      # Color and line mappings
      scale_color_manual(values = color_map, name = "Technology") +
      scale_linetype_manual(values = linetype_map, name = "Sample Type",
                           labels = c("Bulk" = "Bulk", "SC" = "Single-Cell", "SN" = "Single-Nucleus")) +
      scale_shape_manual(values = shape_map, name = "Sample Type",
                        labels = c("Bulk" = "Bulk", "SC" = "Single-Cell", "SN" = "Single-Nucleus")) +
      # Axis formatting
      scale_x_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 5)) +
      scale_y_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 6)) +
      # Labels and theme
      labs(title = paste("Extended Saturation Curves:", feature_name, "- Technology Comparison"),
           subtitle = paste(subtitle_text, "| PacBio (left) vs ONT (right)"),
           x = "Total Reads",
           y = paste("Number of", feature_name, "Detected")) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10, color = "gray50"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.box = "horizontal",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        panel.grid.minor = element_blank(),
        strip.text = element_text(size = 12, face = "bold"),
        strip.background = element_blank()
      ) +
      guides(
        color = guide_legend(title = "Technology", override.aes = list(linewidth = 2)),
        linetype = guide_legend(title = "Sample Type", override.aes = list(linewidth = 2)),
        shape = guide_legend(title = "Sample Type")
      )
    
    # Create unique plot name for faceted version
    plot_name_faceted <- paste0(ft, "_Extended_TechFaceted")
    plot_list[[plot_name_faceted]] <- p_faceted
  }
  
  return(plot_list)
}

# Create combined extended plots
combined_extended_plots <- create_combined_extended_saturation_plots(fitted_models, rarefaction_data, marginal_returns_results)

# Save combined extended plots
if (length(combined_extended_plots) > 0) {
  # Debug: Check what we got
  cat("Combined extended plots created:", length(combined_extended_plots), "\n")
  if (length(combined_extended_plots) > 0) {
    cat("Plot names:", paste(names(combined_extended_plots), collapse = ", "), "\n")
  }
  
  # Separate plots into combined and technology-faceted versions
  combined_plots <- combined_extended_plots[!grepl("_TechFaceted$", names(combined_extended_plots))]
  faceted_plots <- combined_extended_plots[grepl("_TechFaceted$", names(combined_extended_plots))]
  
  cat("Combined plots (non-faceted):", length(combined_plots), "\n")
  cat("Faceted plots:", length(faceted_plots), "\n")
  
  # Use the plots directly without complex reordering that might cause issues
  ordered_combined_plots <- combined_plots
  
  # Skip complex side-by-side comparisons for now to avoid issues
  
  # Save original combined plots (all technologies together) - reordered
  combined_extended_pdf <- file.path(plots_dir, "combined_extended_saturation_curves.pdf")
  
  cat("Saving", length(ordered_combined_plots), "combined plots to:", combined_extended_pdf, "\n")
  cat("Plot order (Isoforms first, then Genes):", paste(names(ordered_combined_plots), collapse = ", "), "\n")
  
  pdf(combined_extended_pdf, width = 12, height = 10)
  
      # Add all the combined plots
    for (plot_name in names(ordered_combined_plots)) {
      cat("Adding plot:", plot_name, "\n")
      print(ordered_combined_plots[[plot_name]])
    }
    
    dev.off()
    
    cat("Combined extended saturation curves saved to:", combined_extended_pdf, "\n")
  
  # Save technology-faceted plots (PacBio vs ONT side-by-side)
  if (length(faceted_plots) > 0) {
    faceted_extended_pdf <- file.path(plots_dir, "combined_extended_saturation_curves_technology_faceted.pdf")
    
    cat("Saving", length(faceted_plots), "technology-faceted plots to:", faceted_extended_pdf, "\n")
    cat("Faceted plot order:", paste(names(faceted_plots), collapse = ", "), "\n")
    
    pdf(faceted_extended_pdf, width = 14, height = 10)  # Wider for side-by-side facets
    for (plot_name in names(faceted_plots)) {
      print(faceted_plots[[plot_name]])
    }
    dev.off()
    
    cat("Technology-faceted extended saturation curves saved to:", faceted_extended_pdf, "\n")
  }
  
  
}

# 3. Model Performance Summary (removed - redundant as user requested)

# 4. Threshold Bar Plots (the ones that were working perfectly at 11:20am)
cat("\n=== CREATING THRESHOLD BAR PLOTS ===\n")

create_threshold_bar_plots <- function(marginal_returns_results) {
  plot_list <- list()
  
  # Group by feature type
  feature_types <- unique(marginal_returns_results$feature_type)
  
  for (ft in feature_types) {
    cat("Creating threshold bar plot for:", ft, "\n")
    
    # Get data for this feature type
    ft_data <- marginal_returns_results %>%
      filter(feature_type == ft) %>%
      mutate(
        protocol = paste(technology, 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         )),
        threshold_label = paste0(threshold, " ", str_remove(ft, " Discovery"), " per Million Reads")
      )
    
    if (nrow(ft_data) == 0) {
      cat("No data found for", ft, "\n")
      next
    }
    
    # Debug: Check what data we have
    cat("Data for", ft, ":\n")
    print(table(ft_data$protocol, ft_data$threshold))
    
    # DEBUG: Check for missing values
    cat("Missing values in features column:\n")
    missing_by_threshold <- ft_data %>%
      group_by(threshold) %>%
      summarise(
        total_protocols = n(),
        missing_features = sum(is.na(features)),
        zero_features = sum(features == 0, na.rm = TRUE),
        .groups = 'drop'
      )
    print(missing_by_threshold)
    
    # Create complete data frame with all protocol-threshold combinations
    # This ensures all protocols appear in each threshold panel
    all_protocols <- unique(ft_data$protocol)
    all_thresholds <- unique(ft_data$threshold)
    
    # Create complete grid - but keep original protocol names as character first
    complete_grid <- expand.grid(
      protocol = as.character(all_protocols),
      threshold = all_thresholds,
      stringsAsFactors = FALSE
    ) %>%
      left_join(ft_data %>% mutate(protocol = as.character(protocol)), 
                by = c("protocol", "threshold")) %>%
      # Fill missing threshold_label
      mutate(
        threshold_label = case_when(
          threshold == 10 ~ paste0("10 ", str_remove(ft, " Discovery"), " per Million Reads"),
          threshold == 1 ~ paste0("1 ", str_remove(ft, " Discovery"), " per Million Reads"), 
          threshold == 0.1 ~ paste0("0.1 ", str_remove(ft, " Discovery"), " per Million Reads"),
          TRUE ~ paste0(threshold, " ", str_remove(ft, " Discovery"), " per Million Reads")
        ),
        # Set features to NA for missing combinations initially (we'll handle this after sorting)
        features = ifelse(is.na(features), NA, features),
        # Extract technology from protocol name for missing entries
        technology = ifelse(is.na(technology), 
                           ifelse(grepl("^PacBio", protocol), "PacBio", "ONT"), 
                           technology)
      )
    
    ft_data <- complete_grid
    
    feature_name <- str_remove(ft, " Discovery")
    
    # Fix threshold ordering (10 at top, 1 in middle, 0.1 at bottom like original)
    ft_data$threshold_label <- factor(ft_data$threshold_label, 
                                     levels = c(paste0("10 ", feature_name, " per Million Reads"), 
                                               paste0("1 ", feature_name, " per Million Reads"), 
                                               paste0("0.1 ", feature_name, " per Million Reads")))
    
    # Simple: fill missing values with 0, then let ggplot sort each panel independently
    ft_data <- ft_data %>%
      mutate(
        features = ifelse(is.na(features), 0, features),
        # Add model stability columns
        features_ci_lower = ifelse(is.na(features_ci_lower), 0, features_ci_lower),
        features_ci_upper = ifelse(is.na(features_ci_upper), features, features_ci_upper),
        model_stability = ifelse(is.na(model_stability), "Unknown", model_stability),
        model_stability = factor(model_stability, levels = c("High", "Moderate", "Low", "Very Low", "Unknown"))
      )
    
    # DEBUG: Check after filling missing values
    cat("After filling missing values with 0:\n")
    filled_summary <- ft_data %>%
      group_by(threshold) %>%
      summarise(
        total_protocols = n(),
        zero_features = sum(features == 0, na.rm = TRUE),
        non_zero_features = sum(features > 0, na.rm = TRUE),
        avg_features = mean(features, na.rm = TRUE),
        .groups = 'drop'
      )
    print(filled_summary)
    
    # Debug the data first - check if threshold logic makes sense
    cat("\nDEBUGGING DATA for", ft, ":\n")
    debug_summary <- ft_data %>%
      group_by(threshold_label) %>%
      summarise(
        min_features = min(features, na.rm = TRUE),
        max_features = max(features, na.rm = TRUE),
        avg_features = mean(features, na.rm = TRUE),
        .groups = 'drop'
      )
    print(debug_summary)
    
    # Create horizontal bar chart with VERTICAL stacking of thresholds (like original)
    # Use reorder_within for proper sorting within facets
    p <- ggplot(ft_data, aes(x = features, y = reorder_within(protocol, features, threshold_label))) +
      # Add error bars first (behind bars)
      geom_errorbarh(aes(xmin = features_ci_lower, xmax = features_ci_upper),
                     height = 0.3, alpha = 0.6, color = "gray30") +
      # Update geom_bar to use alpha for model stability
      geom_bar(aes(fill = technology, alpha = model_stability), 
               stat = "identity", width = 0.7) +
      facet_wrap(~threshold_label, ncol = 1, scales = "free") +  # free both x and y scales
      scale_y_reordered() +  # This is needed for reorder_within
      scale_fill_manual(values = color_map, name = "Technology") +
      # Add alpha scale for model stability (High = solid, Very Low = transparent)
      scale_alpha_manual(values = c("High" = 1, "Moderate" = 0.8, 
                                   "Low" = 0.6, "Very Low" = 0.4, "Unknown" = 0.7),
                        name = "Model Stability",
                        labels = c("High (CV < 5%)", "Moderate (CV 5-15%)", "Low (CV 15-30%)", 
                                  "Very Low (CV > 30%)", "Unknown")) +
      scale_x_continuous(labels = scales::comma, breaks = scales::pretty_breaks(n = 8)) +
      labs(
        title = paste(feature_name, "Detected at Marginal Return Thresholds"),
        subtitle = "Error bars show 95% bootstrap confidence intervals | Transparency indicates model stability",
        x = paste("Total", feature_name, "Detected"),
        y = "Protocol"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 9, angle = 45, hjust = 1),
        axis.title = element_text(size = 11, face = "bold"),
        legend.position = "bottom",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        strip.text = element_text(size = 10, face = "bold"),
        strip.background = element_blank(),
        panel.grid.minor = element_blank()
      )
    
    plot_list[[ft]] <- p
  }
  
  return(plot_list)
}

# Create threshold bar plots
threshold_plots <- create_threshold_bar_plots(marginal_returns_results)

# Save threshold bar plots as combined PDF
if (length(threshold_plots) > 0) {
  combined_threshold_pdf <- file.path(plots_dir, "features_at_thresholds_combined.pdf")
  
  cat("Saving", length(threshold_plots), "threshold plots to combined PDF:", combined_threshold_pdf, "\n")
  
  pdf(combined_threshold_pdf, width = 12, height = 8)
  for (ft in names(threshold_plots)) {
    cat("  Adding", ft, "features threshold plot\n")
    print(threshold_plots[[ft]])
  }
  dev.off()
  
  cat("Combined features threshold plots saved to:", combined_threshold_pdf, "\n")
}

# 4b. Reads Required at Thresholds Bar Plots (COMPLEMENTARY ANALYSIS)
cat("\n=== CREATING READS AT THRESHOLDS BAR PLOTS ===\n")

# Log-transformed version of reads threshold bar plots
create_reads_threshold_bar_plots_log <- function(marginal_returns_results) {
  plot_list <- list()
  
  # Group by feature type
  feature_types <- unique(marginal_returns_results$feature_type)
  
  for (ft in feature_types) {
    cat("Creating log-scale reads threshold bar plots for:", ft, "\n")
    
    # Get data for this feature type
    ft_data <- marginal_returns_results %>%
      filter(feature_type == ft) %>%
      mutate(
        protocol = paste(technology, 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         )),
        threshold_label = paste0(threshold, " ", str_remove(ft, " Discovery"), " per Million Reads"),
        # Convert depth to millions for better readability
        depth_millions = depth / 1e6
      )
    
    if (nrow(ft_data) == 0) {
      cat("No data found for", ft, "\n")
      next
    }
    
    # Create complete data frame with all protocol-threshold combinations
    all_protocols <- unique(ft_data$protocol)
    all_thresholds <- unique(ft_data$threshold)
    
    # Create complete grid
    complete_grid <- expand.grid(
      protocol = as.character(all_protocols),
      threshold = all_thresholds,
      stringsAsFactors = FALSE
    ) %>%
      left_join(ft_data %>% mutate(protocol = as.character(protocol)), 
                by = c("protocol", "threshold")) %>%
      # Fill missing threshold_label and depth_millions
      mutate(
        threshold_label = case_when(
          threshold == 10 ~ paste0("10 ", str_remove(ft, " Discovery"), " per Million Reads"),
          threshold == 1 ~ paste0("1 ", str_remove(ft, " Discovery"), " per Million Reads"), 
          threshold == 0.1 ~ paste0("0.1 ", str_remove(ft, " Discovery"), " per Million Reads"),
          TRUE ~ paste0(threshold, " ", str_remove(ft, " Discovery"), " per Million Reads")
        ),
        # Keep depth as NA for missing combinations
        depth_millions = ifelse(is.na(depth_millions), NA, depth_millions),
        # Extract technology from protocol name for missing entries
        technology = ifelse(is.na(technology), 
                           ifelse(grepl("^PacBio", protocol), "PacBio", "ONT"), 
                           technology),
        # Add model stability columns
        model_stability = ifelse(is.na(model_stability), "Unknown", model_stability),
        model_stability = factor(model_stability, levels = c("High", "Moderate", "Low", "Very Low", "Unknown"))
      )
    
    ft_data <- complete_grid
    
    feature_name <- str_remove(ft, " Discovery")
    
    # Fix threshold ordering (10 at top, 1 in middle, 0.1 at bottom)
    ft_data$threshold_label <- factor(ft_data$threshold_label, 
                                     levels = c(paste0("10 ", feature_name, " per Million Reads"), 
                                               paste0("1 ", feature_name, " per Million Reads"), 
                                               paste0("0.1 ", feature_name, " per Million Reads")))
    
    # Remove rows with missing depth
    ft_data <- ft_data %>%
      filter(!is.na(depth_millions))
    
    # Add estimated reads per cell
    ft_data <- ft_data %>%
      mutate(reads_per_cell = reads_per_cell_converter(depth_millions * 1e6))
    
    # Create Total Reads plot (3 thresholds stacked vertically) - LOG SCALE
    total_reads_plot <- ggplot(ft_data, aes(x = depth_millions, y = reorder_within(protocol, depth_millions, threshold_label))) +
      # Add error bars first (behind bars)
      geom_errorbarh(aes(xmin = depth_ci_lower/1e6, xmax = depth_ci_upper/1e6),
                     height = 0.3, alpha = 0.6, color = "gray30") +
      # Add bars with model stability transparency
      geom_bar(aes(fill = technology, alpha = model_stability), stat = "identity", width = 0.7) +
      facet_wrap(~threshold_label, ncol = 1, scales = "free") +
      scale_y_reordered() +
      scale_fill_manual(values = color_map, name = "Technology") +
      # Add alpha scale for model stability
      scale_alpha_manual(values = c("High" = 1, "Moderate" = 0.8, 
                                   "Low" = 0.6, "Very Low" = 0.4, "Unknown" = 0.7),
                        name = "Model Stability",
                        labels = c("High (CV < 5%)", "Moderate (CV 5-15%)", "Low (CV 15-30%)", 
                                  "Very Low (CV > 30%)", "Unknown")) +
      scale_x_log10(
        name = "Sequencing Depth Required (Million Reads, Log Scale)",
        labels = scales::comma,
        breaks = scales::log_breaks(n = 6)
      ) +
      labs(
        title = paste("Sequencing Depth Required:", feature_name, "- Total Reads (Log Scale)"),
        subtitle = "Error bars show 95% bootstrap confidence intervals | Transparency indicates model stability",
        y = "Protocol"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 9),
        axis.title = element_text(size = 11, face = "bold"),
        legend.position = "bottom",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        strip.text = element_text(size = 10, face = "bold"),
        strip.background = element_blank(),
        panel.grid.major = element_line(color = "grey90", linewidth = 0.5),
        panel.grid.minor = element_line(color = "grey95", linewidth = 0.25)
      )
    
    # Create Estimated Reads per Cell plot (3 thresholds stacked vertically) - LOG SCALE
    reads_per_cell_plot <- ggplot(ft_data, aes(x = reads_per_cell, y = reorder_within(protocol, reads_per_cell, threshold_label))) +
      # Add error bars first (behind bars) - convert depth CI to reads per cell
      geom_errorbarh(aes(xmin = reads_per_cell_converter(depth_ci_lower), 
                         xmax = reads_per_cell_converter(depth_ci_upper)),
                     height = 0.3, alpha = 0.6, color = "gray30") +
      # Add bars with model stability transparency
      geom_bar(aes(fill = technology, alpha = model_stability), stat = "identity", width = 0.7) +
      facet_wrap(~threshold_label, ncol = 1, scales = "free") +
      scale_y_reordered() +
      scale_fill_manual(values = color_map, name = "Technology") +
      # Add alpha scale for model stability
      scale_alpha_manual(values = c("High" = 1, "Moderate" = 0.8, 
                                   "Low" = 0.6, "Very Low" = 0.4, "Unknown" = 0.7),
                        name = "Model Stability",
                        labels = c("High (CV < 5%)", "Moderate (CV 5-15%)", "Low (CV 15-30%)", 
                                  "Very Low (CV > 30%)", "Unknown")) +
      scale_x_log10(
        name = "Estimated Median Reads per Cell (Log Scale)",
        labels = scales::comma,
        breaks = scales::log_breaks(n = 6)
      ) +
      labs(
        title = paste("Sequencing Depth Required:", feature_name, "- Estimated Reads per Cell (Log Scale)"),
        subtitle = "Error bars show 95% bootstrap confidence intervals | Transparency indicates model stability",
        y = "Protocol"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 9),
        axis.title = element_text(size = 11, face = "bold"),
        legend.position = "bottom",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        strip.text = element_text(size = 10, face = "bold"),
        strip.background = element_blank(),
        panel.grid.major = element_line(color = "grey90", linewidth = 0.5),
        panel.grid.minor = element_line(color = "grey95", linewidth = 0.25)
      )
    
    # Add both plots to the list
    plot_list[[paste0(ft, "_TotalReads")]] <- total_reads_plot
    plot_list[[paste0(ft, "_ReadsPerCell")]] <- reads_per_cell_plot
  }
  
  return(plot_list)
}

create_reads_threshold_bar_plots <- function(marginal_returns_results) {
  plot_list <- list()
  
  # Group by feature type
  feature_types <- unique(marginal_returns_results$feature_type)
  
  for (ft in feature_types) {
    cat("Creating reads threshold bar plots for:", ft, "\n")
    
    # Get data for this feature type
    ft_data <- marginal_returns_results %>%
      filter(feature_type == ft) %>%
      mutate(
        protocol = paste(technology, 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         )),
        threshold_label = paste0(threshold, " ", str_remove(ft, " Discovery"), " per Million Reads"),
        # Convert depth to millions for better readability
        depth_millions = depth / 1e6
      )
    
    if (nrow(ft_data) == 0) {
      cat("No data found for", ft, "\n")
      next
    }
    
    # Create complete data frame with all protocol-threshold combinations
    all_protocols <- unique(ft_data$protocol)
    all_thresholds <- unique(ft_data$threshold)
    
    # Create complete grid
    complete_grid <- expand.grid(
      protocol = as.character(all_protocols),
      threshold = all_thresholds,
      stringsAsFactors = FALSE
    ) %>%
      left_join(ft_data %>% mutate(protocol = as.character(protocol)), 
                by = c("protocol", "threshold")) %>%
      # Fill missing threshold_label and depth_millions
      mutate(
        threshold_label = case_when(
          threshold == 10 ~ paste0("10 ", str_remove(ft, " Discovery"), " per Million Reads"),
          threshold == 1 ~ paste0("1 ", str_remove(ft, " Discovery"), " per Million Reads"), 
          threshold == 0.1 ~ paste0("0.1 ", str_remove(ft, " Discovery"), " per Million Reads"),
          TRUE ~ paste0(threshold, " ", str_remove(ft, " Discovery"), " per Million Reads")
        ),
        # Keep depth as NA for missing combinations
        depth_millions = ifelse(is.na(depth_millions), NA, depth_millions),
        # Extract technology from protocol name for missing entries
        technology = ifelse(is.na(technology), 
                           ifelse(grepl("^PacBio", protocol), "PacBio", "ONT"), 
                           technology),
        # Add model stability columns
        model_stability = ifelse(is.na(model_stability), "Unknown", model_stability),
        model_stability = factor(model_stability, levels = c("High", "Moderate", "Low", "Very Low", "Unknown"))
      )
    
    ft_data <- complete_grid
    
    feature_name <- str_remove(ft, " Discovery")
    
    # Fix threshold ordering (10 at top, 1 in middle, 0.1 at bottom)
    ft_data$threshold_label <- factor(ft_data$threshold_label, 
                                     levels = c(paste0("10 ", feature_name, " per Million Reads"), 
                                               paste0("1 ", feature_name, " per Million Reads"), 
                                               paste0("0.1 ", feature_name, " per Million Reads")))
    
    # Remove rows with missing depth
    ft_data <- ft_data %>%
      filter(!is.na(depth_millions))
    
    # Add estimated reads per cell
    ft_data <- ft_data %>%
      mutate(reads_per_cell = reads_per_cell_converter(depth_millions * 1e6))
    
    # Create Total Reads plot (3 thresholds stacked vertically)
    total_reads_plot <- ggplot(ft_data, aes(x = depth_millions, y = reorder_within(protocol, depth_millions, threshold_label))) +
      # Add error bars first (behind bars)
      geom_errorbarh(aes(xmin = depth_ci_lower/1e6, xmax = depth_ci_upper/1e6),
                     height = 0.3, alpha = 0.6, color = "gray30") +
      # Add bars with model stability transparency
      geom_bar(aes(fill = technology, alpha = model_stability), stat = "identity", width = 0.7) +
      facet_wrap(~threshold_label, ncol = 1, scales = "free") +
      scale_y_reordered() +
      scale_fill_manual(values = color_map, name = "Technology") +
      # Add alpha scale for model stability
      scale_alpha_manual(values = c("High" = 1, "Moderate" = 0.8, 
                                   "Low" = 0.6, "Very Low" = 0.4, "Unknown" = 0.7),
                        name = "Model Stability",
                        labels = c("High (CV < 5%)", "Moderate (CV 5-15%)", "Low (CV 15-30%)", 
                                  "Very Low (CV > 30%)", "Unknown")) +
      scale_x_continuous(
        name = "Sequencing Depth Required (Million Reads)",
        labels = scales::comma, 
        breaks = scales::pretty_breaks(n = 8)
      ) +
      labs(
        title = paste("Sequencing Depth Required:", feature_name, "- Total Reads"),
        subtitle = "Error bars show 95% bootstrap confidence intervals | Transparency indicates model stability",
        y = "Protocol"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 9),
        axis.title = element_text(size = 11, face = "bold"),
        legend.position = "bottom",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        strip.text = element_text(size = 10, face = "bold"),
        strip.background = element_blank(),
        panel.grid.major = element_line(color = "grey90", linewidth = 0.5),
        panel.grid.minor = element_line(color = "grey95", linewidth = 0.25)
      )
    
    # Create Estimated Reads per Cell plot (3 thresholds stacked vertically)
    reads_per_cell_plot <- ggplot(ft_data, aes(x = reads_per_cell, y = reorder_within(protocol, reads_per_cell, threshold_label))) +
      # Add error bars first (behind bars) - convert depth CI to reads per cell
      geom_errorbarh(aes(xmin = reads_per_cell_converter(depth_ci_lower), 
                         xmax = reads_per_cell_converter(depth_ci_upper)),
                     height = 0.3, alpha = 0.6, color = "gray30") +
      # Add bars with model stability transparency
      geom_bar(aes(fill = technology, alpha = model_stability), stat = "identity", width = 0.7) +
      facet_wrap(~threshold_label, ncol = 1, scales = "free") +
      scale_y_reordered() +
      scale_fill_manual(values = color_map, name = "Technology") +
      # Add alpha scale for model stability
      scale_alpha_manual(values = c("High" = 1, "Moderate" = 0.8, 
                                   "Low" = 0.6, "Very Low" = 0.4, "Unknown" = 0.7),
                        name = "Model Stability",
                        labels = c("High (CV < 5%)", "Moderate (CV 5-15%)", "Low (CV 15-30%)", 
                                  "Very Low (CV > 30%)", "Unknown")) +
      scale_x_continuous(
        name = "Estimated Median Reads per Cell",
        labels = scales::comma,
        breaks = scales::pretty_breaks(n = 8)
      ) +
      labs(
        title = paste("Sequencing Depth Required:", feature_name, "- Estimated Reads per Cell"),
        subtitle = "Error bars show 95% bootstrap confidence intervals | Transparency indicates model stability",
        y = "Protocol"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 11),
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.text.y = element_text(size = 10),
        axis.text.x = element_text(size = 9),
        axis.title = element_text(size = 11, face = "bold"),
        legend.position = "bottom",
        legend.title = element_text(size = 10, face = "bold"),
        legend.text = element_text(size = 9),
        strip.text = element_text(size = 10, face = "bold"),
        strip.background = element_blank(),
        panel.grid.major = element_line(color = "grey90", linewidth = 0.5),
        panel.grid.minor = element_line(color = "grey95", linewidth = 0.25)
      )
    
    # Add both plots to the list
    plot_list[[paste0(ft, "_TotalReads")]] <- total_reads_plot
    plot_list[[paste0(ft, "_ReadsPerCell")]] <- reads_per_cell_plot
  }
  
  return(plot_list)
}

# Create reads threshold bar plots
reads_threshold_plots <- create_reads_threshold_bar_plots(marginal_returns_results)

# Save reads threshold bar plots as combined PDF (4 pages: Genes Total, Genes Reads per Cell, Isoforms Total, Isoforms Reads per Cell)
if (length(reads_threshold_plots) > 0) {
  combined_reads_threshold_pdf <- file.path(plots_dir, "reads_at_thresholds_combined.pdf")
  
  cat("Saving reads threshold plots in new 4-page structure to:", combined_reads_threshold_pdf, "\n")
  
  # Define the order for consistent 4-page layout
  ordered_plot_names <- c(
    "Genes Discovery_TotalReads",
    "Genes Discovery_ReadsPerCell", 
    "Isoforms Discovery_TotalReads",
    "Isoforms Discovery_ReadsPerCell"
  )
  
  pdf(combined_reads_threshold_pdf, width = 12, height = 12)  # Taller for 3 rows
  
  for (plot_name in ordered_plot_names) {
    if (plot_name %in% names(reads_threshold_plots)) {
      cat("  Adding page:", plot_name, "\n")
      print(reads_threshold_plots[[plot_name]])
    } else {
      cat("  WARNING: Plot not found:", plot_name, "\n")
    }
  }
  
  dev.off()
  
  cat("Combined reads threshold plots saved to:", combined_reads_threshold_pdf, "\n")
  cat("- Page 1: Genes Discovery - Total Reads (3 thresholds: 10, 1, 0.1 f/M)\n")
  cat("- Page 2: Genes Discovery - Estimated Reads per Cell (3 thresholds: 10, 1, 0.1 f/M)\n")
  cat("- Page 3: Isoforms Discovery - Total Reads (3 thresholds: 10, 1, 0.1 f/M)\n")
  cat("- Page 4: Isoforms Discovery - Estimated Reads per Cell (3 thresholds: 10, 1, 0.1 f/M)\n")
  
  # Create log-transformed versions
  cat("\n=== CREATING LOG-TRANSFORMED READS THRESHOLD PLOTS ===\n")
  
  # Create log versions of all plots
  reads_threshold_plots_log <- create_reads_threshold_bar_plots_log(marginal_returns_results)
  
  # Save log-transformed reads threshold plots as separate PDF
  if (length(reads_threshold_plots_log) > 0) {
    combined_reads_threshold_log_pdf <- file.path(plots_dir, "reads_at_thresholds_combined_log.pdf")
    
    cat("Saving log-transformed reads threshold plots to:", combined_reads_threshold_log_pdf, "\n")
    
    pdf(combined_reads_threshold_log_pdf, width = 12, height = 12)  # Taller for 3 rows
    
    for (plot_name in ordered_plot_names) {
      if (plot_name %in% names(reads_threshold_plots_log)) {
        cat("  Adding log-scale page:", plot_name, "\n")
        print(reads_threshold_plots_log[[plot_name]])
      } else {
        cat("  WARNING: Log plot not found:", plot_name, "\n")
      }
    }
    
    dev.off()
    
    cat("Log-transformed reads threshold plots saved to:", combined_reads_threshold_log_pdf, "\n")
    cat("- Page 1: Genes Discovery - Total Reads (Log Scale, 3 thresholds: 10, 1, 0.1 f/M)\n")
    cat("- Page 2: Genes Discovery - Estimated Reads per Cell (Log Scale, 3 thresholds: 10, 1, 0.1 f/M)\n")
    cat("- Page 3: Isoforms Discovery - Total Reads (Log Scale, 3 thresholds: 10, 1, 0.1 f/M)\n")
    cat("- Page 4: Isoforms Discovery - Estimated Reads per Cell (Log Scale, 3 thresholds: 10, 1, 0.1 f/M)\n")
  }
}

# 4c. Marginal Returns Derivative Curves (THE ACTUAL MATHEMATICAL FUNCTIONS!)
cat("\n=== CREATING MARGINAL RETURNS DERIVATIVE CURVES ===\n")

create_marginal_returns_curves <- function(fitted_models, marginal_returns_results) {
  plot_list <- list()
  
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    model_result <- fitted_models$fitted_model[[i]]
    
    if (!model_result$convergence) next
    
    # Get threshold info for this curve
    thresh_info <- marginal_returns_results %>% filter(curve_id == !!curve_id)
    if (nrow(thresh_info) == 0) next
    
    # Extract metadata
    technology <- str_extract(curve_id, "^[^_]+")
    sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
    feature_type <- str_extract(curve_id, "[^_]+$")
    
         # Determine extension range - extend to lowest threshold dynamically
     current_max_depth <- max(model_result$x_values)
     
     # Get lowest threshold depth, fall back to conservative extension if not available
     min_threshold <- min(MARGINAL_THRESHOLDS)
     thresh_min <- thresh_info %>% filter(threshold == min_threshold)
     if (nrow(thresh_min) > 0) {
       extend_to_x <- thresh_min$depth[1] * 1.2  # Extend 20% past lowest threshold
     } else {
       extend_to_x <- current_max_depth * 10  # Conservative fallback
     }
    
    # Get model parameters
    params <- coef(model_result$fit_object)
    model_type <- model_result$model
    
    # First, calculate current marginal returns at current max depth
    current_marginal_returns <- NA
    tryCatch({
      x <- current_max_depth
      
      if (model_type == "michaelis_menten") {
        # f'(x) = (a * b) / (b + x)^2
        a <- params[["a"]] %||% params[[1]]
        b <- params[["b"]] %||% params[[2]]
        derivative <- (a * b) / (b + x)^2
        
      } else if (model_type == "asymptotic_exp") {
        # f'(x) = a * b * exp(-b * x)
        a <- safe_param_get(params, c("a", "a.90%"), params[[1]])
        b <- safe_param_get(params, c("b"), ifelse(length(params) >= 2, params[[2]], 0))
        # Special handling for b ≈ 0 (flat curves)
        if (abs(b) < 1e-10) {
          derivative <- 0.01  # Very small positive derivative for flat curves
        } else {
          derivative <- a * b * exp(-b * x)
        }
        
      } else if (model_type == "power_law") {
        # f'(x) = a * b * x^(b-1)
        a <- params[["a"]] %||% params[[1]]
        b <- params[["b"]] %||% params[[2]]
        if (x > 0) {
          derivative <- a * b * x^(b-1)
        } else {
          derivative <- NA
        }
        
      } else if (model_type == "logarithmic") {
        # f'(x) = a / x
        a <- params[["a"]] %||% params[[1]]
        if (x > 0) {
          derivative <- a / x
        } else {
          derivative <- NA
        }
        
      } else if (model_type == "shifted_logarithmic") {
        # f'(x) = a / (x + c)
        a <- params[["a"]] %||% params[[1]]
        c <- params[["c"]] %||% params[[3]]
        derivative <- a / (x + c)
        
      } else if (model_type == "hill") {
        # f'(x) = (a * n * b^n * x^(n-1)) / (b^n + x^n)^2
        a <- params[["a"]] %||% params[[1]]
        b <- params[["b"]] %||% params[[2]]
        n <- params[["n"]] %||% params[[3]]
        if (x > 0) {
          derivative <- (a * n * b^n * x^(n-1)) / (b^n + x^n)^2
        } else {
          derivative <- NA
        }
      }
      
      # Convert to features per million reads
      current_marginal_returns <- derivative * 1e6
      
    }, error = function(e) {
      current_marginal_returns <- NA
    })
    
    # Create extended x range from current max to plot max
    x_range <- seq(from = current_max_depth, to = extend_to_x, length.out = 1000)
    
    # Calculate derivative (marginal returns) using our existing formulas
    marginal_returns <- rep(NA, length(x_range))
    
    for (j in seq_along(x_range)) {
      x <- x_range[j]
      
      tryCatch({
        if (model_type == "michaelis_menten") {
          # f'(x) = (a * b) / (b + x)^2
          a <- params[["a"]] %||% params[[1]]
          b <- params[["b"]] %||% params[[2]]
          derivative <- (a * b) / (b + x)^2
          
              } else if (model_type == "asymptotic_exp") {
        # f'(x) = a * b * exp(-b * x)
        a <- safe_param_get(params, c("a", "a.90%"), params[[1]])
        b <- safe_param_get(params, c("b"), ifelse(length(params) >= 2, params[[2]], 0))
        # Special handling for b ≈ 0 (flat curves)
        if (abs(b) < 1e-10) {
          derivative <- 0.01  # Very small positive derivative for flat curves
        } else {
          derivative <- a * b * exp(-b * x)
        }
          
        } else if (model_type == "power_law") {
          # f'(x) = a * b * x^(b-1)
          a <- params[["a"]] %||% params[[1]]
          b <- params[["b"]] %||% params[[2]]
          if (x > 0) {
            derivative <- a * b * x^(b-1)
          } else {
            derivative <- NA
          }
          
        } else if (model_type == "logarithmic") {
          # f'(x) = a / x
          a <- params[["a"]] %||% params[[1]]
          if (x > 0) {
            derivative <- a / x
          } else {
            derivative <- NA
          }
          
        } else if (model_type == "shifted_logarithmic") {
          # f'(x) = a / (x + c)
          a <- params[["a"]] %||% params[[1]]
          c <- params[["c"]] %||% params[[3]]
          derivative <- a / (x + c)
          
        } else if (model_type == "hill") {
          # f'(x) = (a * n * b^n * x^(n-1)) / (b^n + x^n)^2
          a <- params[["a"]] %||% params[[1]]
          b <- params[["b"]] %||% params[[2]]
          n <- params[["n"]] %||% params[[3]]
          if (x > 0) {
            derivative <- (a * n * b^n * x^(n-1)) / (b^n + x^n)^2
          } else {
            derivative <- NA
          }
        }
        
        # Convert to features per million reads
        marginal_returns[j] <- derivative * 1e6
        
      }, error = function(e) {
        marginal_returns[j] <- NA
      })
    }
    
    # Create derivative curve data with reads per cell conversion
    derivative_data <- data.frame(
      depth = x_range,
      marginal_returns = marginal_returns,
      reads_per_cell = reads_per_cell_converter(x_range)
    ) %>%
      filter(is.finite(marginal_returns), marginal_returns > 1e-10)  # Use smaller threshold for asymptotic models
    
    # If still no data after filtering, try with even smaller threshold for asymptotic models
    if (nrow(derivative_data) == 0 && model_type == "asymptotic_exp") {
      derivative_data <- data.frame(
        depth = x_range,
        marginal_returns = marginal_returns
      ) %>%
        filter(is.finite(marginal_returns), marginal_returns > 1e-15)
    }
    
    if (nrow(derivative_data) == 0) next
    
    # Create title
    model_type_display <- case_when(
      model_result$model == "michaelis_menten" ~ "Michaelis-Menten",
      model_result$model == "asymptotic_exp" ~ "Asymptotic Exponential",
      model_result$model == "power_law" ~ "Power Law",
      model_result$model == "logarithmic" ~ "Logarithmic",
      model_result$model == "shifted_logarithmic" ~ "Shifted Logarithmic",
      model_result$model == "hill" ~ "Hill Equation",
      TRUE ~ model_result$model
    )
    
    plot_title <- paste0(technology, " ", 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         ), " ", 
                         feature_type, " - Marginal Returns")
    
         # Create subtitle based on whether lowest threshold was found
     if (nrow(thresh_min) > 0) {
       subtitle <- paste0(model_type_display, " | Extended to ", min_threshold, " f/M Threshold")
       extension_label <- paste0("Extended to\n", min_threshold, " f/M")
     } else {
       fold_increase <- round(extend_to_x / current_max_depth, 1)
       subtitle <- paste0(model_type_display, " | Extended ", fold_increase, "x (no ", min_threshold, " f/M threshold)")
       extension_label <- paste0("Extended\n", fold_increase, "x")
     }
    
    # Create the plot
    p <- ggplot() +
      # Add derivative curve
      geom_line(data = derivative_data, 
                aes(x = reads_per_cell, y = marginal_returns),
                color = color_map[technology], 
                linetype = linetype_map[sample_type],
                linewidth = 1.5, alpha = 0.8) +
      
             # Add threshold lines (dynamic based on MARGINAL_THRESHOLDS)
       {lapply(MARGINAL_THRESHOLDS, function(thresh) {
         geom_hline(yintercept = thresh, color = "maroon", linetype = "solid", alpha = 0.7, linewidth = 1)
       })} +
       
       # Add threshold labels (dynamic based on MARGINAL_THRESHOLDS, converted position)
       {lapply(MARGINAL_THRESHOLDS, function(thresh) {
         annotate("text", x = max(derivative_data$reads_per_cell) * 0.95, y = thresh, 
                  label = paste0(thresh, " f/M"), vjust = -0.5, size = 3, color = "maroon", fontface = "bold")
       })} +
      
             # Add vertical lines showing where this protocol reaches each threshold (dynamic, converted)
       {if (nrow(thresh_info) > 0) {
         lapply(MARGINAL_THRESHOLDS, function(thresh) {
           if (any(thresh_info$threshold == thresh)) {
             geom_vline(xintercept = reads_per_cell_converter(thresh_info$depth[thresh_info$threshold == thresh][1]), 
                       linetype = "dotted", alpha = 0.7, color = "maroon")
           }
         })
       }} +
      
      # Add vertical line at current max depth (converted)
      geom_vline(xintercept = reads_per_cell_converter(current_max_depth), linetype = "dashed", alpha = 0.5, color = "gray50") +
      
      # Add horizontal line showing current marginal returns (converted position)
      {if (!is.na(current_marginal_returns)) {
        list(
          geom_hline(yintercept = current_marginal_returns, linetype = "solid", alpha = 0.7, color = "darkblue", linewidth = 1),
          annotate("text", x = max(derivative_data$reads_per_cell) * 0.95, y = current_marginal_returns, 
                   label = paste0(round(current_marginal_returns, 1), " f/M"), vjust = -0.5, size = 3, color = "darkblue", fontface = "bold")
        )
      }} +
      
      # Annotate extension region (converted position)
      annotate("text", x = reads_per_cell_converter(extend_to_x) * 0.8, y = max(derivative_data$marginal_returns) * 0.5, 
               label = extension_label, size = 3, alpha = 0.7, hjust = 0.5) +
      
             # Log scale for better visualization (dynamic based on thresholds and data)
       {
         # Determine y-axis range based on current thresholds and actual data
         min_threshold <- min(MARGINAL_THRESHOLDS)
         max_data <- max(derivative_data$marginal_returns, na.rm = TRUE)
         
         # Set lower limit based on thresholds
         if (min_threshold >= 10) {
           y_lower <- 5
           y_breaks_base <- c(10, 30)
         } else if (min_threshold >= 1) {
           y_lower <- 0.5
           y_breaks_base <- c(1, 3, 10, 30)
         } else {
           y_lower <- 0.05
           y_breaks_base <- c(0.1, 0.3, 1, 3, 10, 30)
         }
         
         # Set upper limit based on actual data (with 20% buffer)
         y_upper <- max_data * 1.2
         
         # Add higher breaks if needed based on data
         if (max_data > 30) {
           if (max_data > 100) {
             if (max_data > 300) {
               y_breaks <- c(y_breaks_base, 100, 300, 1000)
             } else {
               y_breaks <- c(y_breaks_base, 100, 300)
             }
           } else {
             y_breaks <- c(y_breaks_base, 100)
           }
         } else {
           y_breaks <- y_breaks_base
         }
         
         # Filter breaks to only include those within our range
         y_breaks <- y_breaks[y_breaks >= y_lower & y_breaks <= y_upper]
         
         scale_y_log10(labels = scales::comma, breaks = y_breaks, limits = c(y_lower, y_upper))
       } +
      scale_x_continuous(
        name = "Estimated Median Reads per Cell",
        labels = scales::comma, 
        breaks = scales::pretty_breaks(n = 5)
      ) +
      
      # Labels and theme
      labs(title = plot_title,
           subtitle = subtitle,
           y = "Marginal Returns (Features per Million Reads)") +
      theme_minimal() +
      theme(
        text = element_text(size = 10),
        plot.title = element_text(size = 11, face = "bold"),
        plot.subtitle = element_text(size = 9, color = "gray50"),
        axis.text.x = element_text(angle = 45, hjust = 1)
      )
    
    plot_list[[curve_id]] <- p
  }
  
  return(plot_list)
}

# Create marginal returns derivative curves
marginal_returns_curves <- create_marginal_returns_curves(fitted_models, marginal_returns_results)

# Save marginal returns curves as combined PDF with technology grids
if (length(marginal_returns_curves) > 0) {
  # Create technology-grouped grid layouts (same structure as extended curves)
  create_marginal_returns_grids <- function(marginal_returns_curves) {
    # Group plots by technology
    ont_plots <- list()
    pacbio_plots <- list()
    
    for (plot_name in names(marginal_returns_curves)) {
      if (grepl("^ONT", plot_name)) {
        ont_plots[[plot_name]] <- marginal_returns_curves[[plot_name]]
      } else if (grepl("^PacBio", plot_name)) {
        pacbio_plots[[plot_name]] <- marginal_returns_curves[[plot_name]]
      }
    }
    
    # Create grid layouts
    grouped_layouts <- list()
    
    # ONT Technology Grid
    if (length(ont_plots) > 0) {
      sample_order <- c("Bulk", "SC", "SN")
      
      # Find plots for specific positions
      ordered_ont_plots <- list()
      
      # Top row: Genes (Bulk, SC, SN)
      for (sample_type in sample_order) {
        pattern <- paste0("ONT_", sample_type, "_.*Genes Discovery")
        matching_plot <- ont_plots[grepl(pattern, names(ont_plots))]
        if (length(matching_plot) > 0) {
          ordered_ont_plots <- c(ordered_ont_plots, matching_plot)
        }
      }
      
      # Bottom row: Isoforms (Bulk, SC, SN)  
      for (sample_type in sample_order) {
        pattern <- paste0("ONT_", sample_type, "_.*Isoforms Discovery")
        matching_plot <- ont_plots[grepl(pattern, names(ont_plots))]
        if (length(matching_plot) > 0) {
          ordered_ont_plots <- c(ordered_ont_plots, matching_plot)
        }
      }
      
      # Modify plots for grid display
      ont_grid_plots <- lapply(ordered_ont_plots, function(p) {
        p + 
          theme(
            plot.title = element_text(size = 10, face = "bold"),
            plot.subtitle = element_text(size = 8, color = "gray50"),
            axis.title = element_text(size = 9),
            axis.text = element_text(size = 8),
            legend.position = "none"
          )
      })
      
      # Create 2x3 grid layout
      ont_grid <- wrap_plots(ont_grid_plots, ncol = 3, nrow = 2, byrow = TRUE) +
        plot_annotation(
          title = "ONT Marginal Returns Curves",
          subtitle = "Top row: Gene Discovery | Bottom row: Isoform Discovery | Left to right: Bulk, Single-Cell, Single-Nucleus",
          theme = theme(
            plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            plot.subtitle = element_text(size = 10, color = "gray40", hjust = 0.5)
          )
        )
      
      grouped_layouts[["ONT_Grid"]] <- ont_grid
    }
    
    # PacBio Technology Grid  
    if (length(pacbio_plots) > 0) {
      sample_order <- c("Bulk", "SC", "SN")
      
      # Find plots for specific positions
      ordered_pacbio_plots <- list()
      
      # Top row: Genes (Bulk, SC, SN)
      for (sample_type in sample_order) {
        pattern <- paste0("PacBio_", sample_type, "_.*Genes Discovery")
        matching_plot <- pacbio_plots[grepl(pattern, names(pacbio_plots))]
        if (length(matching_plot) > 0) {
          ordered_pacbio_plots <- c(ordered_pacbio_plots, matching_plot)
        }
      }
      
      # Bottom row: Isoforms (Bulk, SC, SN)
      for (sample_type in sample_order) {
        pattern <- paste0("PacBio_", sample_type, "_.*Isoforms Discovery")
        matching_plot <- pacbio_plots[grepl(pattern, names(pacbio_plots))]
        if (length(matching_plot) > 0) {
          ordered_pacbio_plots <- c(ordered_pacbio_plots, matching_plot)
        }
      }
      
      # Modify plots for grid display
      pacbio_grid_plots <- lapply(ordered_pacbio_plots, function(p) {
        p + 
          theme(
            plot.title = element_text(size = 10, face = "bold"),
            plot.subtitle = element_text(size = 8, color = "gray50"),
            axis.title = element_text(size = 9),
            axis.text = element_text(size = 8),
            legend.position = "none"
          )
      })
      
      # Create 2x3 grid layout
      pacbio_grid <- wrap_plots(pacbio_grid_plots, ncol = 3, nrow = 2, byrow = TRUE) +
        plot_annotation(
          title = "PacBio Marginal Returns Curves", 
          subtitle = "Top row: Gene Discovery | Bottom row: Isoform Discovery | Left to right: Bulk, Single-Cell, Single-Nucleus",
          theme = theme(
            plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
            plot.subtitle = element_text(size = 10, color = "gray40", hjust = 0.5)
          )
        )
      
      grouped_layouts[["PacBio_Grid"]] <- pacbio_grid
    }
    
    return(grouped_layouts)
  }
  
  # Create the grouped layouts
  marginal_returns_grids <- create_marginal_returns_grids(marginal_returns_curves)
  
  marginal_returns_curves_pdf <- file.path(plots_dir, "marginal_returns_derivative_curves.pdf")
  
  cat("Saving", length(marginal_returns_curves), "marginal returns curves to:", marginal_returns_curves_pdf, "\n")
  
  pdf(marginal_returns_curves_pdf, width = 12, height = 10)
  
  # First, add technology-grouped grid layouts
  if (length(marginal_returns_grids) > 0) {
    cat("Adding technology-grouped grid layouts to marginal returns PDF...\n")
    for (grid_name in names(marginal_returns_grids)) {
      cat("  Adding", grid_name, "\n")
      print(marginal_returns_grids[[grid_name]])
    }
  }
  
  # Then add individual plots
  cat("Adding individual marginal returns curves...\n")
  for (plot_name in names(marginal_returns_curves)) {
    print(marginal_returns_curves[[plot_name]])
  }
  
  dev.off()
  
  cat("Marginal returns derivative curves saved to:", marginal_returns_curves_pdf, "\n")
  if (length(marginal_returns_grids) > 0) {
    cat("- Includes technology-grouped grid layouts at the start\n")
    for (grid_name in names(marginal_returns_grids)) {
      cat("  *", grid_name, "\n")
    }
  }
  
  # Generate additional versions with different threshold combinations
  cat("\n=== GENERATING ADDITIONAL THRESHOLD VERSIONS ===\n")
  
  # Save original thresholds
  original_thresholds <- MARGINAL_THRESHOLDS
  
  # Define threshold combinations for additional versions
  threshold_versions <- list(
    "no_0.1" = c(10, 1),           # Remove 0.1, keep 1 and 10
    "only_10" = c(10)              # Remove 0.1 and 1, keep only 10
  )
  
  for (version_name in names(threshold_versions)) {
    cat("Generating version:", version_name, "with thresholds:", paste(threshold_versions[[version_name]], collapse = ", "), "\n")
    
    # Set new thresholds
    MARGINAL_THRESHOLDS <<- threshold_versions[[version_name]]
    
         # Regenerate marginal returns curves with new thresholds
     cat("  Regenerating marginal returns curves...\n")
     marginal_returns_curves_version <- create_marginal_returns_curves(fitted_models, marginal_returns_results)
    
    # Create grids for this version
    marginal_returns_grids_version <- create_marginal_returns_grids(marginal_returns_curves_version)
    
    # Generate PDF with version-specific filename
    version_pdf <- file.path(plots_dir, paste0("marginal_returns_derivative_curves_", version_name, ".pdf"))
    
    cat("  Saving to:", version_pdf, "\n")
    
    pdf(version_pdf, width = 12, height = 10)
    
    # Add technology-grouped grid layouts
    if (length(marginal_returns_grids_version) > 0) {
      for (grid_name in names(marginal_returns_grids_version)) {
        print(marginal_returns_grids_version[[grid_name]])
      }
    }
    
    # Add individual plots
    for (plot_name in names(marginal_returns_curves_version)) {
      print(marginal_returns_curves_version[[plot_name]])
    }
    
    dev.off()
    
    cat("  Version", version_name, "saved successfully\n")
  }
  
  # Restore original thresholds
  MARGINAL_THRESHOLDS <<- original_thresholds
  
  cat("Additional threshold versions generated:\n")
  cat("  - marginal_returns_derivative_curves_no_0.1.pdf (thresholds: 10, 1 f/M)\n")
  cat("  - marginal_returns_derivative_curves_only_10.pdf (thresholds: 10 f/M only)\n")
}

# Bootstrap Distribution Visualization
cat("\n=== CREATING BOOTSTRAP DISTRIBUTION PLOTS ===\n")

create_bootstrap_distribution_plots <- function(fitted_models) {
  plot_list <- list()
  
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    bootstrap_results <- fitted_models$bootstrap_results[[i]]
    
    if (length(bootstrap_results) == 0) next
    
    technology <- str_extract(curve_id, "^[^_]+")
    sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
    feature_type <- str_extract(curve_id, "[^_]+$")
    
    # Combine bootstrap results for all thresholds
    boot_data <- data.frame()
    
    for (thresh in names(bootstrap_results)) {
      boot_res <- bootstrap_results[[thresh]]
      if (!is.null(boot_res$all_depths)) {
        temp_df <- data.frame(
          depth = boot_res$all_depths / 1e6,  # Convert to millions
          threshold = as.numeric(thresh),
          threshold_label = paste0(thresh, " f/M")
        )
        boot_data <- rbind(boot_data, temp_df)
      }
    }
    
    if (nrow(boot_data) == 0) next
    
    # Add statistics for each threshold
    stats_data <- boot_data %>%
      group_by(threshold_label) %>%
      summarise(
        median = median(depth),
        lower = quantile(depth, 0.025),
        upper = quantile(depth, 0.975),
        cv = sd(depth) / mean(depth),
        .groups = 'drop'
      ) %>%
      mutate(
        label = paste0("Median: ", round(median, 1), "M\n",
                      "95% CI: [", round(lower, 1), "-", round(upper, 1), "M]\n",
                      "CV: ", round(cv * 100, 1), "%")
      )
    
    plot_title <- paste0(technology, " ", 
                         case_when(
                           sample_type == "SC" ~ "Single-Cell",
                           sample_type == "SN" ~ "Single-Nucleus",
                           TRUE ~ sample_type
                         ), " ", 
                         feature_type)
    
    p <- ggplot(boot_data, aes(x = depth)) +
      geom_histogram(aes(y = after_stat(density)), 
                    bins = 30, fill = color_map[technology], 
                    alpha = 0.7, color = "black") +
      geom_density(color = "black", linewidth = 1) +
      geom_vline(data = stats_data, aes(xintercept = median), 
                color = "red", linewidth = 1.5, linetype = "solid") +
      geom_vline(data = stats_data, aes(xintercept = lower), 
                color = "red", linewidth = 1, linetype = "dashed") +
      geom_vline(data = stats_data, aes(xintercept = upper), 
                color = "red", linewidth = 1, linetype = "dashed") +
      geom_text(data = stats_data, 
               aes(x = Inf, y = Inf, label = label),
               hjust = 1.1, vjust = 1.1, size = 3) +
      facet_wrap(~threshold_label, scales = "free") +
      labs(
        title = paste0("Bootstrap Distributions: ", plot_title),
        subtitle = paste0("Based on ", N_BOOTSTRAP, " bootstrap samples"),
        x = "Sequencing Depth (Million Reads)",
        y = "Density"
      ) +
      theme_minimal() +
      theme(
        text = element_text(size = 10),
        plot.title = element_text(size = 12, face = "bold"),
        plot.subtitle = element_text(size = 9, color = "gray50"),
        strip.text = element_text(size = 10, face = "bold"),
        strip.background = element_blank()
      )
    
    plot_list[[curve_id]] <- p
  }
  
  return(plot_list)
}

# Create and save bootstrap distribution plots
bootstrap_dist_plots <- create_bootstrap_distribution_plots(fitted_models_with_uncertainty)

if (length(bootstrap_dist_plots) > 0) {
  bootstrap_dist_pdf <- file.path(plots_dir, "bootstrap_distributions.pdf")
  
  pdf(bootstrap_dist_pdf, width = 12, height = 8)
  for (curve_id in names(bootstrap_dist_plots)) {
    print(bootstrap_dist_plots[[curve_id]])
  }
  dev.off()
  
  cat("Bootstrap distribution plots saved to:", bootstrap_dist_pdf, "\n")
}

# 5. Extension Summary Table
cat("\n=== CREATING EXTENSION SUMMARY ===\n")

# 5. Protocol Threshold Summary Table
cat("\n=== CREATING PROTOCOL THRESHOLD SUMMARY ===\n")

# Create summary tables for all thresholds
create_threshold_summary <- function(threshold_value) {
  marginal_returns_results %>%
    filter(threshold == threshold_value) %>%
    mutate(
      protocol = paste(technology, sample_type),
      efficiency_score = features / log10(depth),  # Features per order of magnitude
      estimated_reads_per_cell = reads_per_cell_converter(depth),
      depth_millions = round(depth / 1e6, 1),
      estimated_reads_per_cell_rounded = round(estimated_reads_per_cell, 0)
    ) %>%
    select(protocol, feature_type, model_type, depth_millions, estimated_reads_per_cell_rounded, 
           features, fold_increase_from_current, efficiency_score, 
           depth_ci_lower, depth_ci_upper, depth_cv, features_ci_lower, features_ci_upper, 
           bootstrap_convergence_rate, extrapolation_factor, model_stability) %>%
    arrange(feature_type, desc(efficiency_score))
}

# Create summaries for all thresholds
protocol_summary_10fm <- create_threshold_summary(10)
protocol_summary_1fm <- create_threshold_summary(1)
protocol_summary_0.1fm <- create_threshold_summary(0.1)

# Note: Protocol threshold summary data is generated for report inclusion only (no separate CSV files)

# Create nice formatted tables for the report
create_formatted_summary_table <- function(summary_data, threshold_name) {
  summary_data %>%
    mutate(
      # Include confidence intervals where available
      depth_with_ci = ifelse(!is.na(depth_ci_lower), 
                            paste0(depth_millions, "M [", 
                                  round(depth_ci_lower/1e6, 1), "-", 
                                  round(depth_ci_upper/1e6, 1), "]"),
                            paste0(depth_millions, "M")),
      features_with_ci = ifelse(!is.na(features_ci_lower),
                               paste0(round(features), " [", 
                                     round(features_ci_lower), "-", 
                                     round(features_ci_upper), "]"),
                               scales::comma(round(features))),
      reads_per_cell_formatted = scales::comma(estimated_reads_per_cell_rounded),
      fold_increase_formatted = paste0(round(fold_increase_from_current, 1), "x"),
      efficiency_formatted = round(efficiency_score, 1),
      cv_percent = ifelse(!is.na(depth_cv), paste0(round(depth_cv * 100, 1), "%"), ""),
      stability_formatted = ifelse(!is.na(model_stability), as.character(model_stability), "")
    ) %>%
    select(protocol, feature_type, depth_with_ci, reads_per_cell_formatted, 
           features_with_ci, fold_increase_formatted, efficiency_formatted, 
           cv_percent, stability_formatted) %>%
          rename(
        "Protocol" = protocol,
        "Feature Type" = feature_type,
        "Depth (M reads) [95% CI]" = depth_with_ci,
        "Est. Reads per Cell" = reads_per_cell_formatted,
        "Features [95% CI]" = features_with_ci,
        "Fold Increase" = fold_increase_formatted,
        "Efficiency Score" = efficiency_formatted,
        "CV" = cv_percent,
        "Model Stability" = stability_formatted
      )
}

summary_table_10fm <- create_formatted_summary_table(protocol_summary_10fm, "10")
summary_table_1fm <- create_formatted_summary_table(protocol_summary_1fm, "1")
summary_table_0.1fm <- create_formatted_summary_table(protocol_summary_0.1fm, "0.1")

# Print summaries
cat("\n=== SUMMARY: Features at 10 features/million reads threshold ===\n")
print(summary_table_10fm)

cat("\n=== SUMMARY: Features at 1 feature/million reads threshold ===\n")
print(summary_table_1fm)

cat("\n=== SUMMARY: Features at 0.1 features/million reads threshold ===\n")
print(summary_table_0.1fm)

## Model Equations Report -------
cat("\n=== GENERATING MODEL EQUATIONS REPORT ===\n")

# Create comprehensive model equations report
create_model_equations_report <- function(fitted_models) {
  equation_lines <- c(
    "# Model Equations Report",
    "",
    paste("Date:", Sys.Date()),
    paste("Total curves analyzed:", nrow(fitted_models)),
    "",
    "## Mathematical Model Definitions",
    "",
    "### Michaelis-Menten Model",
    "Function: f(x) = (a * x) / (b + x)",
    "Derivative: f'(x) = (a * b) / (b + x)²",
    "Parameters: a = asymptotic maximum, b = half-saturation constant",
    "",
    "### Asymptotic Exponential Model", 
    "Function: f(x) = a * (1 - exp(-b * x))",
    "Derivative: f'(x) = a * b * exp(-b * x)",
    "Parameters: a = asymptotic maximum, b = growth rate",
    "",
    "### Power Law Model",
    "Function: f(x) = a * x^b",
    "Derivative: f'(x) = a * b * x^(b-1)",
    "Parameters: a = scaling coefficient, b = power exponent",
    "",
    "### Logarithmic Model",
    "Function: f(x) = a * log(x) + b",
    "Derivative: f'(x) = a / x",
    "Parameters: a = slope coefficient, b = intercept",
    "",
    "### Shifted Logarithmic Model",
    "Function: f(x) = a * log(x + c) + b",
    "Derivative: f'(x) = a / (x + c)",
    "Parameters: a = slope coefficient, b = intercept, c = shift parameter",
    "",
    "### Hill Equation Model",
    "Function: f(x) = (a * x^n) / (b^n + x^n)",
    "Derivative: f'(x) = (a * n * b^n * x^(n-1)) / (b^n + x^n)²",
    "Parameters: a = maximum response, b = half-saturation, n = Hill coefficient",
    "",
    "## Fitted Model Parameters by Curve",
    ""
  )
  
  # Add individual curve equations with fitted parameters
  for (i in 1:nrow(fitted_models)) {
    curve_id <- fitted_models$curve_id[i]
    model_result <- fitted_models$fitted_model[[i]]
    
    if (!model_result$convergence) {
      equation_lines <- c(equation_lines, 
                         paste("###", curve_id),
                         "Model: FAILED TO CONVERGE",
                         "")
      next
    }
    
    # Get model parameters
    params <- coef(model_result$fit_object)
    model_type <- model_result$model
    r_squared <- model_result$r_squared
    
    # Extract metadata
    technology <- str_extract(curve_id, "^[^_]+")
    sample_type <- str_extract(curve_id, "(?<=_)[^_]+(?=_)")
    feature_type <- str_extract(curve_id, "[^_]+$")
    
    equation_lines <- c(equation_lines, 
                       paste("###", curve_id),
                       paste("Technology:", technology),
                       paste("Sample Type:", sample_type),
                       paste("Feature Type:", feature_type),
                       paste("Model Type:", model_type),
                       paste("R-squared:", round(r_squared, 4)),
                       "")
    
    # Add specific equations with fitted parameters
    if (model_type == "michaelis_menten") {
      a <- params[["a"]] %||% params[[1]]
      b <- params[["b"]] %||% params[[2]]
      equation_lines <- c(equation_lines,
                         paste("Parameters: a =", round(a, 4), ", b =", round(b, 4)),
                         paste("Fitted Function: f(x) = (", round(a, 4), "* x) / (", round(b, 4), "+ x)"),
                         paste("Derivative: f'(x) = (", round(a, 4), "*", round(b, 4), ") / (", round(b, 4), "+ x)²"),
                         paste("Marginal Returns: f'(x) * 1,000,000 features per million reads"))
      
    } else if (model_type == "asymptotic_exp") {
      a <- safe_param_get(params, c("a", "a.90%"), params[[1]])
      b <- safe_param_get(params, c("b"), ifelse(length(params) >= 2, params[[2]], 0))
      equation_lines <- c(equation_lines,
                         paste("Parameters: a =", round(a, 4), ", b =", format(b, scientific = TRUE, digits = 4)),
                         paste("Fitted Function: f(x) = ", round(a, 4), "* (1 - exp(-", format(b, scientific = TRUE, digits = 4), "* x))"),
                         paste("Derivative: f'(x) = ", round(a, 4), "*", format(b, scientific = TRUE, digits = 4), "* exp(-", format(b, scientific = TRUE, digits = 4), "* x)"),
                         paste("Marginal Returns: f'(x) * 1,000,000 features per million reads"))
      
      if (abs(b) < 1e-10) {
        equation_lines <- c(equation_lines, "NOTE: b ≈ 0, curve is essentially flat with minimal marginal returns")
      }
      
    } else if (model_type == "power_law") {
      a <- params[["a"]] %||% params[[1]]
      b <- params[["b"]] %||% params[[2]]
      equation_lines <- c(equation_lines,
                         paste("Parameters: a =", format(a, scientific = TRUE, digits = 4), ", b =", round(b, 4)),
                         paste("Fitted Function: f(x) = ", format(a, scientific = TRUE, digits = 4), "* x^", round(b, 4)),
                         paste("Derivative: f'(x) = ", format(a, scientific = TRUE, digits = 4), "*", round(b, 4), "* x^", round(b-1, 4)),
                         paste("Marginal Returns: f'(x) * 1,000,000 features per million reads"))
      
    } else if (model_type == "logarithmic") {
      a <- params[["a"]] %||% params[[1]]
      b <- params[["b"]] %||% params[[2]]
      equation_lines <- c(equation_lines,
                         paste("Parameters: a =", round(a, 4), ", b =", round(b, 4)),
                         paste("Fitted Function: f(x) = ", round(a, 4), "* log(x) +", round(b, 4)),
                         paste("Derivative: f'(x) = ", round(a, 4), "/ x"),
                         paste("Marginal Returns: f'(x) * 1,000,000 features per million reads"))
      
    } else if (model_type == "shifted_logarithmic") {
      a <- params[["a"]] %||% params[[1]]
      b <- params[["b"]] %||% params[[2]]
      c <- params[["c"]] %||% params[[3]]
      equation_lines <- c(equation_lines,
                         paste("Parameters: a =", round(a, 4), ", b =", round(b, 4), ", c =", round(c, 4)),
                         paste("Fitted Function: f(x) = ", round(a, 4), "* log(x +", round(c, 4), ") +", round(b, 4)),
                         paste("Derivative: f'(x) = ", round(a, 4), "/ (x +", round(c, 4), ")"),
                         paste("Marginal Returns: f'(x) * 1,000,000 features per million reads"))
      
    } else if (model_type == "hill") {
      a <- params[["a"]] %||% params[[1]]
      b <- params[["b"]] %||% params[[2]]
      n <- params[["n"]] %||% params[[3]]
      equation_lines <- c(equation_lines,
                         paste("Parameters: a =", round(a, 4), ", b =", round(b, 4), ", n =", round(n, 4)),
                         paste("Fitted Function: f(x) = (", round(a, 4), "* x^", round(n, 4), ") / (", round(b, 4), "^", round(n, 4), "+ x^", round(n, 4), ")"),
                         paste("Derivative: f'(x) = (", round(a, 4), "*", round(n, 4), "*", round(b, 4), "^", round(n, 4), "* x^", round(n-1, 4), ") / (", round(b, 4), "^", round(n, 4), "+ x^", round(n, 4), ")²"),
                         paste("Marginal Returns: f'(x) * 1,000,000 features per million reads"))
      
    } else if (model_type == "linear_fallback") {
      slope <- coef(model_result$fit_object)[2]
      intercept <- coef(model_result$fit_object)[1]
      equation_lines <- c(equation_lines,
                         paste("Parameters: slope =", round(slope, 4), ", intercept =", round(intercept, 4)),
                         paste("Fitted Function: f(x) = ", round(slope, 4), "* x +", round(intercept, 4)),
                         paste("Derivative: f'(x) = ", round(slope, 4), "(constant)"),
                         paste("Marginal Returns: ", round(slope * 1e6, 4), "features per million reads (constant)"),
                         "NOTE: Linear fallback model used due to nonlinear model convergence failure")
    }
    
    equation_lines <- c(equation_lines, "", "")
  }
  
  # Add summary statistics
  equation_lines <- c(equation_lines,
                     "## Model Type Summary",
                     "")
  
  model_summary <- table(fitted_models$model_type)
  for (model_name in names(model_summary)) {
    equation_lines <- c(equation_lines, 
                       paste("-", model_name, ":", model_summary[model_name], "curves"))
  }
  
  equation_lines <- c(equation_lines,
                     "",
                     "## Notes",
                     "- x represents total sequencing reads",
                     "- f(x) represents number of features detected", 
                     "- f'(x) represents the derivative (rate of feature discovery per additional read)",
                     "- Marginal returns are calculated as f'(x) * 1,000,000 to express features per million additional reads",
                     "- Thresholds (10 f/M, 1 f/M, 0.1 f/M) represent points where marginal returns drop to these levels",
                     "- Lower marginal returns indicate diminishing efficiency of additional sequencing")
  
  return(equation_lines)
}

# Generate model equations report
model_equations_lines <- create_model_equations_report(fitted_models)
writeLines(model_equations_lines, file.path(stats_dir, "model_equations_report.txt"))

cat("Model equations report saved to:", file.path(stats_dir, "model_equations_report.txt"), "\n")

## Summary Report -------
cat("\n=== GENERATING SUMMARY REPORT ===\n")

# Extract conversion model details for report
conversion_model <- attr(reads_per_cell_converter, "model")
conversion_r_squared <- attr(reads_per_cell_converter, "r_squared")
conversion_intercept <- attr(reads_per_cell_converter, "intercept")
conversion_slope <- attr(reads_per_cell_converter, "slope")

# Count data points used for conversion model
cell_data_count <- rarefaction_data %>%
  filter(!is.na(median_reads_per_cell), median_reads_per_cell > 0) %>%
  nrow()

# Create text report
report_lines <- c(
  "# Rarefaction Curve Marginal Returns Analysis Report",
  "",
  paste("Date:", Sys.Date()),
  paste("Total curves analyzed:", nrow(fitted_models)),
  "",
  "## Model Selection",
  capture.output(table(fitted_models$model_type)),
  "",
  "## Marginal Return Thresholds Used",
  paste("- ", MARGINAL_THRESHOLDS, "features per million reads"),
  "",
  "## Reads per Cell Conversion Model",
  paste("Data points used for model fitting:", cell_data_count),
  paste("Model equation: median_reads_per_cell =", round(conversion_intercept, 3), "+", round(conversion_slope, 6), "* total_reads"),
  paste("Model R-squared:", round(conversion_r_squared, 4)),
  paste("Model quality:", ifelse(conversion_r_squared >= 0.9, "Excellent (R² ≥ 0.9)", 
                                ifelse(conversion_r_squared >= 0.8, "Good (R² ≥ 0.8)",
                                      ifelse(conversion_r_squared >= 0.7, "Fair (R² ≥ 0.7)", "Poor (R² < 0.7)")))),
  "",
  "## Summary at 10 features/million reads threshold",
  knitr::kable(summary_table_10fm, format = "simple"),
  "",
  "## Summary at 1 feature/million reads threshold",
  knitr::kable(summary_table_1fm, format = "simple"),
  "",
  "## Summary at 0.1 features/million reads threshold",
  knitr::kable(summary_table_0.1fm, format = "simple"),
  "",
  "## Interpretation",
  "- Total Reads: sequencing depth (in millions) where marginal returns drop below threshold",
  "- Est. Reads per Cell: estimated median reads per cell at the threshold depth",
  "- Features Detected: total features found at that sequencing depth",
  "- Fold Increase: multiplier of current sequencing depth needed to reach threshold", 
  "- Efficiency Score: features per order of magnitude of sequencing (higher is better)",
  "- Model Stability: consistency of predictions based on coefficient of variation (CV)",
  "- CV: standard deviation / mean of bootstrap predictions (lower = more stable)",
  "",
  "## Threshold Definitions",
  "- 10 f/M: relatively efficient detection (10 features per million additional reads)",
  "- 1 f/M: reasonable diminishing returns point (1 feature per million additional reads)",
  "- 0.1 f/M: severe diminishing returns (only 0.1 features per million additional reads)",
  "",
  "## Bootstrap Analysis Summary",
  "",
  paste("Bootstrap iterations:", N_BOOTSTRAP),
  paste("Confidence level:", CONFIDENCE_LEVEL * 100, "%"),
  "",
  "## Model Stability Categories",
  "- High: CV < 5% (predictions highly consistent)",
  "- Moderate: CV 5-15% (predictions reasonably consistent)", 
  "- Low: CV 15-30% (predictions moderately variable)",
  "- Very Low: CV > 30% (predictions highly variable)",
  "",
  "## Reads per Cell Estimation",
  "Estimated reads per cell values are calculated using a linear regression model fitted to",
  "single-cell and single-nucleus data where actual median reads per cell are available.",
  "",
  "## Bootstrap Methodology",
  "Bootstrap confidence intervals are generated by resampling the 5-10 rarefaction curve",
  "data points with replacement, refitting the mathematical model, and recalculating",
  "threshold depths. Model stability reflects how consistently the mathematical model",
  "produces the same predictions across bootstrap samples, independent of extrapolation distance.",
  ""
)

writeLines(report_lines, file.path(stats_dir, "marginal_returns_analysis_report.txt"))

cat("\n=== ANALYSIS COMPLETE ===\n")
cat("Results saved in:", stats_dir, "\n")
cat("\nCONFIGURATION: Plot-specific color schemes can be modified in PLOT_COLOR_SCHEMES at top of script\n")
cat("- Conversion plot uses RED+BLUE for overlap detection\n")
cat("- All other plots use standard colors\n")
cat("\nKey outputs:\n")
cat("- Model fits with bootstrap: fitted_models_with_bootstrap.rds\n")
cat("- Model fits (original): fitted_models_marginal_returns.rds\n")
cat("- Marginal returns analysis with uncertainty: marginal_returns_analysis.csv\n")
cat("- Threshold summaries with confidence intervals: included in marginal_returns_analysis_report.txt\n")
cat("- Plots directory: plots/\n")
cat("  - Reads conversion model: reads_conversion_model.pdf\n")
cat("    * Scatter plot of total reads vs measured reads per cell with fitted linear regression\n")
cat("    * Shows equation, R-squared, and model quality for all SC/SN data points\n")
cat("    * Uses RED+BLUE color scheme for optimal overlap detection\n")
cat("  - Individual model fits: individual_model_fits_with_rsquared.pdf\n")
cat("  - Individual model fits (reads per cell): individual_model_fits_reads_per_cell.pdf\n")
cat("  - Extended saturation curves: extended_saturation_curves.pdf\n")
cat("    * Technology-grouped grid layouts (ONT, PacBio) with individual plots\n")
cat("    * Extended to 1 f/M threshold where applicable\n")
cat("    * X-axis: Estimated Median Reads per Cell (single axis, no dual axis)\n")
cat("  - Combined extended curves: combined_extended_saturation_curves.pdf\n")
cat("    * Extended to furthest 1 f/M threshold for each feature type\n")
cat("  - Technology-faceted curves: combined_extended_saturation_curves_technology_faceted.pdf\n")
cat("  - Combined threshold plots: features_at_thresholds_combined.pdf\n")
cat("    * Page 1: Genes Discovery | Page 2: Isoforms Discovery\n")
cat("    * Horizontal bar charts with error bars showing 95% bootstrap confidence intervals\n")
cat("    * Features detected at 10, 1, 0.1 f/M thresholds with model stability indicators\n")
cat("  - Combined reads threshold plots: reads_at_thresholds_combined.pdf\n")
cat("    * 4 pages: Genes Total Reads, Genes Reads per Cell, Isoforms Total Reads, Isoforms Reads per Cell\n")
cat("    * Each page shows 3 thresholds (10, 1, 0.1 f/M) with horizontal bars and 95% confidence intervals\n")
cat("    * Error bars show bootstrap uncertainty, transparency indicates model stability\n")
cat("    * Protocols sorted by efficiency (most efficient leftmost)\n")
cat("  - Combined reads threshold plots (Log Scale): reads_at_thresholds_combined_log.pdf\n")
cat("    * Same 4 pages as above but with log-transformed x-axis for better visualization of fold differences\n")
cat("    * Log scale useful when there are large differences between protocols\n")
cat("  - Marginal returns derivative curves: marginal_returns_derivative_curves.pdf\n")
cat("    * Technology-grouped grid layouts (ONT, PacBio) with individual plots\n")
cat("    * Extended to 0.1 f/M threshold where applicable\n")
cat("    * Individual mathematical derivative curves showing actual marginal returns decline\n")
cat("    * X-axis: Estimated Median Reads per Cell (single axis, no dual axis)\n")
cat("  - Bootstrap distribution plots: bootstrap_distributions.pdf\n")
cat("    * Histograms and density plots showing bootstrap distributions\n")
cat("    * Confidence intervals and coefficient of variation for threshold estimates\n")
cat("    * Model stability quantification for all protocols and thresholds\n")
cat("- Reports:\n")
cat("  * Marginal returns analysis: marginal_returns_analysis_report.txt\n")
cat("  * Model equations and derivatives: model_equations_report.txt\n")